{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell contains all needed imports for feature engineering section and models creating sections, \n",
    "#this is due to possible error that may occur if some imports wouldn't be added\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.colors as mcolors\n",
    "import networkx as nx\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import gensim\n",
    "import spacy\n",
    "import nltk\n",
    "import ast\n",
    "\n",
    "from spacy import load\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy import stats\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.tag import pos_tag\n",
    "from scipy.stats import mannwhitneyu, mood, ks_2samp, pearsonr, pointbiserialr\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from feature_engine.selection import SelectBySingleFeaturePerformance\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nrclex import NRCLex\n",
    "from collections import Counter\n",
    "from afinn import Afinn\n",
    "\n",
    "#Needed imports for models section\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data sets after feature enginnering for building models\n",
    "\n",
    "tweets_df= pd.read_csv('tweets_df_after_second_step.csv')\n",
    "dev_val_df= pd.read_csv('dev_val_df_after_second_step.csv')\n",
    "team_validation= pd.read_csv('team_validation_after_second_step.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "additional_features = tweets_df[['topic_0','topic_1','topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9', 'topic_10', 'topic_11', 'topic_12', 'topic_13',\n",
    "       'topic_14', 'topic_15', 'topic_16', 'topic_17', 'topic_18', 'topic_19', 'topic_20', 'topic_21', 'topic_22', 'topic_23', 'topic_24', 'topic_25', 'topic_26', 'topic_27', 'topic_28', 'topic_29', 'topic_30', 'topic_31', 'topic_32', 'topic_33', 'topic_34', 'words_counted', 'polarity', 'subjectivity', 'negative', 'neutral', 'positive', 'compound', 'afinn_score', 'complex', 'negation', 'Hour', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'Night', 'WorkHours', 'Evening', 'fear', 'anticipation', 'trust', 'surprise', 'sadness', 'joy', 'exclamation_count', 'question_count', 'uppercase_count', 'count_RB', 'count_VBG', 'count_VBD', 'count_VBZ', 'count_RBR', 'count_JJR', 'count_VB', 'count_IN', 'count_VBN', 'count_UH', 'count_NN', 'count_NNS', 'count_NNP', 'count_MD', 'count_JJS', 'count_FW', 'count_PRP', 'count_VBP', 'count_JJ', 'count_DT', 'count_CD', 'words_counted_first_negative', 'words_counted_second_negative', 'contains_RB_JJ', 'contains_RB_NN',\n",
    "        'sum_positive_corr', 'sum_negative_corr','fear_anticipation', 'fear_trust', 'fear_surprise', 'fear_sadness', 'fear_joy', 'anticipation_trust', 'anticipation_surprise', 'anticipation_sadness', 'anticipation_joy', 'trust_surprise', 'trust_sadness', 'trust_joy', 'surprise_sadness', 'surprise_joy', 'sadness_joy']]\n",
    "y = tweets_df['Target']\n",
    "\n",
    "\n",
    "# Normalizacja danych\n",
    "scaler = StandardScaler()\n",
    "additional_features_scaled = scaler.fit_transform(additional_features)\n",
    "\n",
    "#setting train data set\n",
    "X_train= additional_features_scaled\n",
    "y_train= y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "additional_features = dev_val_df[['topic_0','topic_1','topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9', 'topic_10', 'topic_11', 'topic_12', 'topic_13',\n",
    "       'topic_14', 'topic_15', 'topic_16', 'topic_17', 'topic_18', 'topic_19', 'topic_20', 'topic_21', 'topic_22', 'topic_23', 'topic_24', 'topic_25', 'topic_26', 'topic_27', 'topic_28', 'topic_29', 'topic_30', 'topic_31', 'topic_32', 'topic_33', 'topic_34', 'words_counted', 'polarity', 'subjectivity', 'negative', 'neutral', 'positive', 'compound', 'afinn_score', 'complex', 'negation', 'Hour', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'Night', 'WorkHours', 'Evening', 'fear', 'anticipation', 'trust', 'surprise', 'sadness', 'joy', 'exclamation_count', 'question_count', 'uppercase_count', 'count_RB', 'count_VBG', 'count_VBD', 'count_VBZ', 'count_RBR', 'count_JJR', 'count_VB', 'count_IN', 'count_VBN', 'count_UH', 'count_NN', 'count_NNS', 'count_NNP', 'count_MD', 'count_JJS', 'count_FW', 'count_PRP', 'count_VBP', 'count_JJ', 'count_DT', 'count_CD', 'words_counted_first_negative', 'words_counted_second_negative', 'contains_RB_JJ', 'contains_RB_NN',\n",
    "        'sum_positive_corr', 'sum_negative_corr','fear_anticipation', 'fear_trust', 'fear_surprise', 'fear_sadness', 'fear_joy', 'anticipation_trust', 'anticipation_surprise', 'anticipation_sadness', 'anticipation_joy', 'trust_surprise', 'trust_sadness', 'trust_joy', 'surprise_sadness', 'surprise_joy', 'sadness_joy']]\n",
    "y = dev_val_df['Target']\n",
    "\n",
    "\n",
    "# data normalization\n",
    "scaler = StandardScaler()\n",
    "additional_features_scaled2 = scaler.fit_transform(additional_features)\n",
    "\n",
    "#setting train data set\n",
    "X_test= additional_features_scaled2\n",
    "y_test= y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "additional_features = team_validation[['topic_0','topic_1','topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9', 'topic_10', 'topic_11', 'topic_12', 'topic_13',\n",
    "       'topic_14', 'topic_15', 'topic_16', 'topic_17', 'topic_18', 'topic_19', 'topic_20', 'topic_21', 'topic_22', 'topic_23', 'topic_24', 'topic_25', 'topic_26', 'topic_27', 'topic_28', 'topic_29', 'topic_30', 'topic_31', 'topic_32', 'topic_33', 'topic_34', 'words_counted', 'polarity', 'subjectivity', 'negative', 'neutral', 'positive', 'compound', 'afinn_score', 'complex', 'negation', 'Hour', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'Night', 'WorkHours', 'Evening', 'fear', 'anticipation', 'trust', 'surprise', 'sadness', 'joy', 'exclamation_count', 'question_count', 'uppercase_count', 'count_RB', 'count_VBG', 'count_VBD', 'count_VBZ', 'count_RBR', 'count_JJR', 'count_VB', 'count_IN', 'count_VBN', 'count_UH', 'count_NN', 'count_NNS', 'count_NNP', 'count_MD', 'count_JJS', 'count_FW', 'count_PRP', 'count_VBP', 'count_JJ', 'count_DT', 'count_CD', 'words_counted_first_negative', 'words_counted_second_negative', 'contains_RB_JJ', 'contains_RB_NN',\n",
    "        'sum_positive_corr', 'sum_negative_corr','fear_anticipation', 'fear_trust', 'fear_surprise', 'fear_sadness', 'fear_joy', 'anticipation_trust', 'anticipation_surprise', 'anticipation_sadness', 'anticipation_joy', 'trust_surprise', 'trust_sadness', 'trust_joy', 'surprise_sadness', 'surprise_joy', 'sadness_joy']]\n",
    "y = team_validation['Target']\n",
    "\n",
    "\n",
    "\n",
    "additional_features= additional_features.loc[:, ~additional_features.columns.duplicated()]\n",
    "\n",
    "y = team_validation['Target']\n",
    "\n",
    "# data normalization\n",
    "scaler = StandardScaler()\n",
    "additional_features_scaled = scaler.fit_transform(additional_features)\n",
    "\n",
    "#setting train data set\n",
    "X_TEST_TEAM= additional_features_scaled\n",
    "y_TEST_TEAM= y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba cech w danych treningowych: 107\n",
      "Liczba cech w danych testowych: 107\n"
     ]
    }
   ],
   "source": [
    "print(\"Liczba cech w danych treningowych:\", X_train.shape[1])\n",
    "print(\"Liczba cech w danych testowych:\", X_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net number 1, many hidden layers, wihout balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25176/25176 [==============================] - 58s 2ms/step - loss: 0.3996 - accuracy: 0.8178 - val_loss: 0.3880 - val_accuracy: 0.8203\n",
      "Epoch 2/10\n",
      "25176/25176 [==============================] - 56s 2ms/step - loss: 0.3855 - accuracy: 0.8241 - val_loss: 0.3992 - val_accuracy: 0.8191\n",
      "Epoch 3/10\n",
      "25176/25176 [==============================] - 57s 2ms/step - loss: 0.3820 - accuracy: 0.8251 - val_loss: 0.4023 - val_accuracy: 0.7935\n",
      "Epoch 4/10\n",
      "25176/25176 [==============================] - 54s 2ms/step - loss: 0.3786 - accuracy: 0.8265 - val_loss: 0.3854 - val_accuracy: 0.8144\n",
      "Epoch 5/10\n",
      "25176/25176 [==============================] - 57s 2ms/step - loss: 0.3758 - accuracy: 0.8282 - val_loss: 0.3837 - val_accuracy: 0.8221\n",
      "Epoch 6/10\n",
      "25176/25176 [==============================] - 54s 2ms/step - loss: 0.3741 - accuracy: 0.8295 - val_loss: 0.3816 - val_accuracy: 0.8245\n",
      "Epoch 7/10\n",
      "25176/25176 [==============================] - 54s 2ms/step - loss: 0.3739 - accuracy: 0.8297 - val_loss: 0.3813 - val_accuracy: 0.8269\n",
      "Epoch 8/10\n",
      "25176/25176 [==============================] - 58s 2ms/step - loss: 0.3727 - accuracy: 0.8301 - val_loss: 0.3794 - val_accuracy: 0.8253\n",
      "Epoch 9/10\n",
      "25176/25176 [==============================] - 58s 2ms/step - loss: 0.3699 - accuracy: 0.8321 - val_loss: 0.3850 - val_accuracy: 0.8285\n",
      "Epoch 10/10\n",
      "25176/25176 [==============================] - 56s 2ms/step - loss: 0.3702 - accuracy: 0.8321 - val_loss: 0.3772 - val_accuracy: 0.8226\n",
      "3372/3372 [==============================] - 4s 1ms/step - loss: 0.3804 - accuracy: 0.8226\n",
      "Test accuracy: [0.3804216980934143, 0.8226456642150879]\n",
      "3372/3372 [==============================] - 4s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89     82343\n",
      "           1       0.75      0.37      0.50     25554\n",
      "\n",
      "    accuracy                           0.82    107897\n",
      "   macro avg       0.79      0.67      0.70    107897\n",
      "weighted avg       0.81      0.82      0.80    107897\n",
      "\n",
      "[[79256  3087]\n",
      " [16049  9505]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# building neural network model\n",
    "model_nn = Sequential([\n",
    "    Dense(512,activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.0),\n",
    "    Dense(1, activation='sigmoid')  # using sigmoid for binary classification\n",
    "])\n",
    "\n",
    "# model compilation with additional metrics\n",
    "model_nn.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model training\n",
    "history = model_nn.fit(X_train, y_train, epochs=10, batch_size=8, validation_split=0.2)\n",
    "\n",
    "# model validation of test set\n",
    "accuracy = model_nn.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test accuracy: {accuracy}')\n",
    "      \n",
    "# additional mark with the usage of sklearn  (for example classification report)\n",
    "y_pred = (model_nn.predict(X_test) > 0.50).astype(\"int32\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net number 2,  one hidden layer, without balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "25176/25176 [==============================] - 26s 1ms/step - loss: 0.3874 - accuracy: 0.8211 - val_loss: 0.3835 - val_accuracy: 0.8237\n",
      "Epoch 2/7\n",
      "25176/25176 [==============================] - 24s 969us/step - loss: 0.3753 - accuracy: 0.8266 - val_loss: 0.3777 - val_accuracy: 0.8257\n",
      "Epoch 3/7\n",
      "25176/25176 [==============================] - 24s 972us/step - loss: 0.3701 - accuracy: 0.8296 - val_loss: 0.3743 - val_accuracy: 0.8266\n",
      "Epoch 4/7\n",
      "25176/25176 [==============================] - 25s 990us/step - loss: 0.3665 - accuracy: 0.8305 - val_loss: 0.3750 - val_accuracy: 0.8277\n",
      "Epoch 5/7\n",
      "25176/25176 [==============================] - 28s 1ms/step - loss: 0.3641 - accuracy: 0.8320 - val_loss: 0.3722 - val_accuracy: 0.8255\n",
      "Epoch 6/7\n",
      "25176/25176 [==============================] - 25s 1ms/step - loss: 0.3613 - accuracy: 0.8330 - val_loss: 0.3725 - val_accuracy: 0.8270\n",
      "Epoch 7/7\n",
      "25176/25176 [==============================] - 25s 1ms/step - loss: 0.3596 - accuracy: 0.8345 - val_loss: 0.3712 - val_accuracy: 0.8293\n",
      "3372/3372 [==============================] - 3s 760us/step - loss: 0.3740 - accuracy: 0.8265\n",
      "Test accuracy: [0.37400585412979126, 0.8265382647514343]\n",
      "3372/3372 [==============================] - 2s 669us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89     82343\n",
      "           1       0.67      0.53      0.59     25554\n",
      "\n",
      "    accuracy                           0.83    107897\n",
      "   macro avg       0.77      0.72      0.74    107897\n",
      "weighted avg       0.82      0.83      0.82    107897\n",
      "\n",
      "[[75747  6596]\n",
      " [12120 13434]]\n"
     ]
    }
   ],
   "source": [
    "model_nn = Sequential([\n",
    "    Dense(256,activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(1, activation='sigmoid')  # UÅ¼ywamy sigmoid dla klasyfikacji binarnej\n",
    "])\n",
    "\n",
    "# Compilation of the model with additional metrics\n",
    "model_nn.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Model training\n",
    "history = model_nn.fit(X_train, y_train, epochs=7 , batch_size=8, validation_split=0.2)\n",
    "\n",
    "# Model evaluation on the test set\n",
    "accuracy = model_nn.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test accuracy: {accuracy}')\n",
    "      \n",
    "# Additional evaluation via sklearn (e.g. classification report)\n",
    "y_pred = (model_nn.predict(X_test) > 0.50).astype(\"int32\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the RandomUnderSampler object\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Transforming data using undersampling\n",
    "X_train_balanced, y_train_balanced = rus.fit_resample(additional_features_scaled, tweets_df['Target'])\n",
    "\n",
    "X_test_balanced, y_test_balanced = rus.fit_resample(additional_features_scaled2, dev_val_df['Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer with not balanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- here we are preprocessing and preparing data set for the use with BERT(Bidirectional Encoder Representations from Transformers) model, <br>\n",
    "we are using `bert-base-uncased`. <span style=\"color:lightblue\">BERT</span> is a advanced language model developed by Google in 2018. <br>\n",
    "It is one form of a transformer which are used to understand and generate human language text. This model is<br>\n",
    "is provided petrained and what we do is <bold>fine-tuning</bold>. In this phase BERT is trained on much smaller data set, which is directly related to the<br>\n",
    "task for which the model is to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\igorr\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#preparing data in advance so that it is understandable by the model\n",
    "\n",
    "\n",
    "#This line initializes a tokenizer for the 'bert-base-uncased' model. \n",
    "#The tokenizer is used to convert text data into a format that the BERT model can understand (i.e., tokenizing the text).\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization and data preparation\n",
    "\n",
    "#These lists will store the tokenized input data and the attention masks, respectively.\n",
    "#The attention mask tells the model which parts of the input are actual data and which are padding.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "#Here, each tweet is tokenized using encode_plus, which also adds special tokens (like [CLS] and [SEP])\n",
    "\n",
    "for tweet in tweets_df['Text']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tweet,\n",
    "                        add_special_tokens = True,  \n",
    "                        max_length = 64,          \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',     \n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "#These lines convert lists of input_ids and attention_masks into tensors,\n",
    "#and also prepare a tensor of labels from your dataset.\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(tweets_df['Target'].values)\n",
    "\n",
    "# Creating dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "#data loader is a data set whose value is added to the model in batches of size 32\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and preparation of data for a new validation set\n",
    "\n",
    "#below we do the same things as decribed above, the only difference is that we overwrite `validation_dataloader` object\n",
    "new_input_ids = []\n",
    "new_attention_masks = []\n",
    "\n",
    "for tweet in dev_val_df['Text']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tweet,\n",
    "                        add_special_tokens = True,  \n",
    "                        max_length = 64,           \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',    \n",
    "                   )\n",
    "    \n",
    "    new_input_ids.append(encoded_dict['input_ids'])\n",
    "    new_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "new_input_ids = torch.cat(new_input_ids, dim=0)\n",
    "new_attention_masks = torch.cat(new_attention_masks, dim=0)\n",
    "new_labels = torch.tensor(dev_val_df['Target'].values)\n",
    "\n",
    "#changing the validation set\n",
    "new_val_dataset = TensorDataset(new_input_ids, new_attention_masks, new_labels)\n",
    "\n",
    "# updating the validation data loader to contain data from right data set\n",
    "validation_dataloader = DataLoader(\n",
    "            new_val_dataset,\n",
    "            sampler = SequentialSampler(new_val_dataset),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "#to even consider fine-tuning we have to make sure that we have CUDA installed. Is reduces training time. In our case using CUDA for\n",
    "#training neural network on GPU was 8 times more faster, then training neural network on CPU.\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in the below section we are preparing <span style=\"color:lightblue\">BERT</span> model to statiment classification. <br>\n",
    "`BertForSequenceClassification.from_pretrained` has already been pre-trained on a large corpus of general language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial preparation of the model\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels = 2,  # Two classes: positive and negative\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "# Preparation for training (example for CPU, this needs to be adapted to GPU)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\igorr\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Trening...\n",
      "  Partia    40  z  7,081.    Czas: 0:00:21.\n",
      "  Partia    80  z  7,081.    Czas: 0:00:36.\n",
      "  Partia   120  z  7,081.    Czas: 0:00:51.\n",
      "  Partia   160  z  7,081.    Czas: 0:01:06.\n",
      "  Partia   200  z  7,081.    Czas: 0:01:20.\n",
      "  Partia   240  z  7,081.    Czas: 0:01:35.\n",
      "  Partia   280  z  7,081.    Czas: 0:01:50.\n",
      "  Partia   320  z  7,081.    Czas: 0:02:05.\n",
      "  Partia   360  z  7,081.    Czas: 0:02:20.\n",
      "  Partia   400  z  7,081.    Czas: 0:02:34.\n",
      "  Partia   440  z  7,081.    Czas: 0:02:47.\n",
      "  Partia   480  z  7,081.    Czas: 0:03:00.\n",
      "  Partia   520  z  7,081.    Czas: 0:03:14.\n",
      "  Partia   560  z  7,081.    Czas: 0:03:27.\n",
      "  Partia   600  z  7,081.    Czas: 0:03:41.\n",
      "  Partia   640  z  7,081.    Czas: 0:03:55.\n",
      "  Partia   680  z  7,081.    Czas: 0:04:09.\n",
      "  Partia   720  z  7,081.    Czas: 0:04:22.\n",
      "  Partia   760  z  7,081.    Czas: 0:04:36.\n",
      "  Partia   800  z  7,081.    Czas: 0:04:49.\n",
      "  Partia   840  z  7,081.    Czas: 0:05:03.\n",
      "  Partia   880  z  7,081.    Czas: 0:05:16.\n",
      "  Partia   920  z  7,081.    Czas: 0:05:31.\n",
      "  Partia   960  z  7,081.    Czas: 0:05:46.\n",
      "  Partia 1,000  z  7,081.    Czas: 0:06:01.\n",
      "  Partia 1,040  z  7,081.    Czas: 0:06:16.\n",
      "  Partia 1,080  z  7,081.    Czas: 0:06:30.\n",
      "  Partia 1,120  z  7,081.    Czas: 0:06:44.\n",
      "  Partia 1,160  z  7,081.    Czas: 0:06:58.\n",
      "  Partia 1,200  z  7,081.    Czas: 0:07:25.\n",
      "  Partia 1,240  z  7,081.    Czas: 0:07:38.\n",
      "  Partia 1,280  z  7,081.    Czas: 0:07:52.\n",
      "  Partia 1,320  z  7,081.    Czas: 0:08:06.\n",
      "  Partia 1,360  z  7,081.    Czas: 0:08:20.\n",
      "  Partia 1,400  z  7,081.    Czas: 0:08:34.\n",
      "  Partia 1,440  z  7,081.    Czas: 0:08:48.\n",
      "  Partia 1,480  z  7,081.    Czas: 0:09:02.\n",
      "  Partia 1,520  z  7,081.    Czas: 0:09:15.\n",
      "  Partia 1,560  z  7,081.    Czas: 0:09:30.\n",
      "  Partia 1,600  z  7,081.    Czas: 0:09:44.\n",
      "  Partia 1,640  z  7,081.    Czas: 0:09:57.\n",
      "  Partia 1,680  z  7,081.    Czas: 0:10:18.\n",
      "  Partia 1,720  z  7,081.    Czas: 0:10:41.\n",
      "  Partia 1,760  z  7,081.    Czas: 0:10:56.\n",
      "  Partia 1,800  z  7,081.    Czas: 0:11:10.\n",
      "  Partia 1,840  z  7,081.    Czas: 0:11:24.\n",
      "  Partia 1,880  z  7,081.    Czas: 0:11:38.\n",
      "  Partia 1,920  z  7,081.    Czas: 0:11:52.\n",
      "  Partia 1,960  z  7,081.    Czas: 0:12:06.\n",
      "  Partia 2,000  z  7,081.    Czas: 0:12:20.\n",
      "  Partia 2,040  z  7,081.    Czas: 0:12:34.\n",
      "  Partia 2,080  z  7,081.    Czas: 0:12:48.\n",
      "  Partia 2,120  z  7,081.    Czas: 0:13:17.\n",
      "  Partia 2,160  z  7,081.    Czas: 0:13:31.\n",
      "  Partia 2,200  z  7,081.    Czas: 0:13:45.\n",
      "  Partia 2,240  z  7,081.    Czas: 0:13:59.\n",
      "  Partia 2,280  z  7,081.    Czas: 0:14:13.\n",
      "  Partia 2,320  z  7,081.    Czas: 0:14:27.\n",
      "  Partia 2,360  z  7,081.    Czas: 0:14:40.\n",
      "  Partia 2,400  z  7,081.    Czas: 0:14:54.\n",
      "  Partia 2,440  z  7,081.    Czas: 0:15:20.\n",
      "  Partia 2,480  z  7,081.    Czas: 0:15:37.\n",
      "  Partia 2,520  z  7,081.    Czas: 0:15:51.\n",
      "  Partia 2,560  z  7,081.    Czas: 0:16:07.\n",
      "  Partia 2,600  z  7,081.    Czas: 0:16:21.\n",
      "  Partia 2,640  z  7,081.    Czas: 0:16:36.\n",
      "  Partia 2,680  z  7,081.    Czas: 0:16:50.\n",
      "  Partia 2,720  z  7,081.    Czas: 0:17:21.\n",
      "  Partia 2,760  z  7,081.    Czas: 0:17:35.\n",
      "  Partia 2,800  z  7,081.    Czas: 0:17:49.\n",
      "  Partia 2,840  z  7,081.    Czas: 0:18:02.\n",
      "  Partia 2,880  z  7,081.    Czas: 0:18:16.\n",
      "  Partia 2,920  z  7,081.    Czas: 0:18:30.\n",
      "  Partia 2,960  z  7,081.    Czas: 0:18:44.\n",
      "  Partia 3,000  z  7,081.    Czas: 0:18:58.\n",
      "  Partia 3,040  z  7,081.    Czas: 0:19:12.\n",
      "  Partia 3,080  z  7,081.    Czas: 0:19:26.\n",
      "  Partia 3,120  z  7,081.    Czas: 0:19:57.\n",
      "  Partia 3,160  z  7,081.    Czas: 0:20:11.\n",
      "  Partia 3,200  z  7,081.    Czas: 0:20:26.\n",
      "  Partia 3,240  z  7,081.    Czas: 0:20:41.\n",
      "  Partia 3,280  z  7,081.    Czas: 0:20:55.\n",
      "  Partia 3,320  z  7,081.    Czas: 0:21:09.\n",
      "  Partia 3,360  z  7,081.    Czas: 0:21:23.\n",
      "  Partia 3,400  z  7,081.    Czas: 0:21:55.\n",
      "  Partia 3,440  z  7,081.    Czas: 0:22:08.\n",
      "  Partia 3,480  z  7,081.    Czas: 0:22:22.\n",
      "  Partia 3,520  z  7,081.    Czas: 0:22:36.\n",
      "  Partia 3,560  z  7,081.    Czas: 0:22:50.\n",
      "  Partia 3,600  z  7,081.    Czas: 0:23:04.\n",
      "  Partia 3,640  z  7,081.    Czas: 0:23:18.\n",
      "  Partia 3,680  z  7,081.    Czas: 0:23:32.\n",
      "  Partia 3,720  z  7,081.    Czas: 0:23:46.\n",
      "  Partia 3,760  z  7,081.    Czas: 0:24:16.\n",
      "  Partia 3,800  z  7,081.    Czas: 0:24:30.\n",
      "  Partia 3,840  z  7,081.    Czas: 0:24:43.\n",
      "  Partia 3,880  z  7,081.    Czas: 0:24:57.\n",
      "  Partia 3,920  z  7,081.    Czas: 0:25:11.\n",
      "  Partia 3,960  z  7,081.    Czas: 0:25:25.\n",
      "  Partia 4,000  z  7,081.    Czas: 0:25:39.\n",
      "  Partia 4,040  z  7,081.    Czas: 0:26:11.\n",
      "  Partia 4,080  z  7,081.    Czas: 0:26:25.\n",
      "  Partia 4,120  z  7,081.    Czas: 0:26:39.\n",
      "  Partia 4,160  z  7,081.    Czas: 0:26:53.\n",
      "  Partia 4,200  z  7,081.    Czas: 0:27:07.\n",
      "  Partia 4,240  z  7,081.    Czas: 0:27:21.\n",
      "  Partia 4,280  z  7,081.    Czas: 0:27:35.\n",
      "  Partia 4,320  z  7,081.    Czas: 0:28:07.\n",
      "  Partia 4,360  z  7,081.    Czas: 0:28:20.\n",
      "  Partia 4,400  z  7,081.    Czas: 0:28:34.\n",
      "  Partia 4,440  z  7,081.    Czas: 0:28:48.\n",
      "  Partia 4,480  z  7,081.    Czas: 0:29:02.\n",
      "  Partia 4,520  z  7,081.    Czas: 0:29:16.\n",
      "  Partia 4,560  z  7,081.    Czas: 0:29:30.\n",
      "  Partia 4,600  z  7,081.    Czas: 0:30:02.\n",
      "  Partia 4,640  z  7,081.    Czas: 0:30:17.\n",
      "  Partia 4,680  z  7,081.    Czas: 0:30:31.\n",
      "  Partia 4,720  z  7,081.    Czas: 0:30:46.\n",
      "  Partia 4,760  z  7,081.    Czas: 0:31:02.\n",
      "  Partia 4,800  z  7,081.    Czas: 0:31:35.\n",
      "  Partia 4,840  z  7,081.    Czas: 0:31:49.\n",
      "  Partia 4,880  z  7,081.    Czas: 0:32:05.\n",
      "  Partia 4,920  z  7,081.    Czas: 0:32:19.\n",
      "  Partia 4,960  z  7,081.    Czas: 0:32:33.\n",
      "  Partia 5,000  z  7,081.    Czas: 0:32:47.\n",
      "  Partia 5,040  z  7,081.    Czas: 0:33:20.\n",
      "  Partia 5,080  z  7,081.    Czas: 0:33:34.\n",
      "  Partia 5,120  z  7,081.    Czas: 0:33:48.\n",
      "  Partia 5,160  z  7,081.    Czas: 0:34:01.\n",
      "  Partia 5,200  z  7,081.    Czas: 0:34:15.\n",
      "  Partia 5,240  z  7,081.    Czas: 0:34:29.\n",
      "  Partia 5,280  z  7,081.    Czas: 0:34:43.\n",
      "  Partia 5,320  z  7,081.    Czas: 0:35:11.\n",
      "  Partia 5,360  z  7,081.    Czas: 0:35:30.\n",
      "  Partia 5,400  z  7,081.    Czas: 0:35:44.\n",
      "  Partia 5,440  z  7,081.    Czas: 0:35:58.\n",
      "  Partia 5,480  z  7,081.    Czas: 0:36:12.\n",
      "  Partia 5,520  z  7,081.    Czas: 0:36:26.\n",
      "  Partia 5,560  z  7,081.    Czas: 0:36:39.\n",
      "  Partia 5,600  z  7,081.    Czas: 0:37:04.\n",
      "  Partia 5,640  z  7,081.    Czas: 0:37:25.\n",
      "  Partia 5,680  z  7,081.    Czas: 0:37:38.\n",
      "  Partia 5,720  z  7,081.    Czas: 0:37:52.\n",
      "  Partia 5,760  z  7,081.    Czas: 0:38:06.\n",
      "  Partia 5,800  z  7,081.    Czas: 0:38:22.\n",
      "  Partia 5,840  z  7,081.    Czas: 0:38:53.\n",
      "  Partia 5,880  z  7,081.    Czas: 0:39:07.\n",
      "  Partia 5,920  z  7,081.    Czas: 0:39:20.\n",
      "  Partia 5,960  z  7,081.    Czas: 0:39:35.\n",
      "  Partia 6,000  z  7,081.    Czas: 0:39:49.\n",
      "  Partia 6,040  z  7,081.    Czas: 0:40:19.\n",
      "  Partia 6,080  z  7,081.    Czas: 0:40:33.\n",
      "  Partia 6,120  z  7,081.    Czas: 0:40:47.\n",
      "  Partia 6,160  z  7,081.    Czas: 0:41:01.\n",
      "  Partia 6,200  z  7,081.    Czas: 0:41:15.\n",
      "  Partia 6,240  z  7,081.    Czas: 0:41:28.\n",
      "  Partia 6,280  z  7,081.    Czas: 0:42:00.\n",
      "  Partia 6,320  z  7,081.    Czas: 0:42:14.\n",
      "  Partia 6,360  z  7,081.    Czas: 0:42:28.\n",
      "  Partia 6,400  z  7,081.    Czas: 0:42:42.\n",
      "  Partia 6,440  z  7,081.    Czas: 0:42:56.\n",
      "  Partia 6,480  z  7,081.    Czas: 0:43:09.\n",
      "  Partia 6,520  z  7,081.    Czas: 0:43:23.\n",
      "  Partia 6,560  z  7,081.    Czas: 0:43:54.\n",
      "  Partia 6,600  z  7,081.    Czas: 0:44:08.\n",
      "  Partia 6,640  z  7,081.    Czas: 0:44:21.\n",
      "  Partia 6,680  z  7,081.    Czas: 0:44:35.\n",
      "  Partia 6,720  z  7,081.    Czas: 0:44:49.\n",
      "  Partia 6,760  z  7,081.    Czas: 0:45:03.\n",
      "  Partia 6,800  z  7,081.    Czas: 0:45:16.\n",
      "  Partia 6,840  z  7,081.    Czas: 0:45:43.\n",
      "  Partia 6,880  z  7,081.    Czas: 0:46:03.\n",
      "  Partia 6,920  z  7,081.    Czas: 0:46:17.\n",
      "  Partia 6,960  z  7,081.    Czas: 0:46:31.\n",
      "  Partia 7,000  z  7,081.    Czas: 0:46:45.\n",
      "  Partia 7,040  z  7,081.    Czas: 0:46:59.\n",
      "  Partia 7,080  z  7,081.    Czas: 0:47:13.\n",
      "\n",
      "  Årednia strata treningu: 0.31\n",
      "  Trening epoki trwaÅ: 0:47:13\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Trening...\n",
      "  Partia    40  z  7,081.    Czas: 0:00:14.\n",
      "  Partia    80  z  7,081.    Czas: 0:00:48.\n",
      "  Partia   120  z  7,081.    Czas: 0:01:02.\n",
      "  Partia   160  z  7,081.    Czas: 0:01:16.\n",
      "  Partia   200  z  7,081.    Czas: 0:01:29.\n",
      "  Partia   240  z  7,081.    Czas: 0:01:44.\n",
      "  Partia   280  z  7,081.    Czas: 0:02:08.\n",
      "  Partia   320  z  7,081.    Czas: 0:02:30.\n",
      "  Partia   360  z  7,081.    Czas: 0:02:43.\n",
      "  Partia   400  z  7,081.    Czas: 0:02:56.\n",
      "  Partia   440  z  7,081.    Czas: 0:03:09.\n",
      "  Partia   480  z  7,081.    Czas: 0:03:22.\n",
      "  Partia   520  z  7,081.    Czas: 0:03:35.\n",
      "  Partia   560  z  7,081.    Czas: 0:04:07.\n",
      "  Partia   600  z  7,081.    Czas: 0:04:20.\n",
      "  Partia   640  z  7,081.    Czas: 0:04:32.\n",
      "  Partia   680  z  7,081.    Czas: 0:04:45.\n",
      "  Partia   720  z  7,081.    Czas: 0:04:58.\n",
      "  Partia   760  z  7,081.    Czas: 0:05:12.\n",
      "  Partia   800  z  7,081.    Czas: 0:05:43.\n",
      "  Partia   840  z  7,081.    Czas: 0:05:59.\n",
      "  Partia   880  z  7,081.    Czas: 0:06:15.\n",
      "  Partia   920  z  7,081.    Czas: 0:06:30.\n",
      "  Partia   960  z  7,081.    Czas: 0:07:01.\n",
      "  Partia 1,000  z  7,081.    Czas: 0:07:16.\n",
      "  Partia 1,040  z  7,081.    Czas: 0:07:31.\n",
      "  Partia 1,080  z  7,081.    Czas: 0:07:45.\n",
      "  Partia 1,120  z  7,081.    Czas: 0:08:16.\n",
      "  Partia 1,160  z  7,081.    Czas: 0:08:29.\n",
      "  Partia 1,200  z  7,081.    Czas: 0:08:42.\n",
      "  Partia 1,240  z  7,081.    Czas: 0:08:55.\n",
      "  Partia 1,280  z  7,081.    Czas: 0:09:09.\n",
      "  Partia 1,320  z  7,081.    Czas: 0:09:22.\n",
      "  Partia 1,360  z  7,081.    Czas: 0:09:35.\n",
      "  Partia 1,400  z  7,081.    Czas: 0:10:06.\n",
      "  Partia 1,440  z  7,081.    Czas: 0:10:18.\n",
      "  Partia 1,480  z  7,081.    Czas: 0:10:31.\n",
      "  Partia 1,520  z  7,081.    Czas: 0:10:43.\n",
      "  Partia 1,560  z  7,081.    Czas: 0:10:57.\n",
      "  Partia 1,600  z  7,081.    Czas: 0:11:11.\n",
      "  Partia 1,640  z  7,081.    Czas: 0:11:38.\n",
      "  Partia 1,680  z  7,081.    Czas: 0:11:55.\n",
      "  Partia 1,720  z  7,081.    Czas: 0:12:08.\n",
      "  Partia 1,760  z  7,081.    Czas: 0:12:21.\n",
      "  Partia 1,800  z  7,081.    Czas: 0:12:34.\n",
      "  Partia 1,840  z  7,081.    Czas: 0:12:47.\n",
      "  Partia 1,880  z  7,081.    Czas: 0:13:00.\n",
      "  Partia 1,920  z  7,081.    Czas: 0:13:23.\n",
      "  Partia 1,960  z  7,081.    Czas: 0:13:47.\n",
      "  Partia 2,000  z  7,081.    Czas: 0:14:00.\n",
      "  Partia 2,040  z  7,081.    Czas: 0:14:12.\n",
      "  Partia 2,080  z  7,081.    Czas: 0:14:25.\n",
      "  Partia 2,120  z  7,081.    Czas: 0:14:40.\n",
      "  Partia 2,160  z  7,081.    Czas: 0:14:55.\n",
      "  Partia 2,200  z  7,081.    Czas: 0:15:27.\n",
      "  Partia 2,240  z  7,081.    Czas: 0:15:41.\n",
      "  Partia 2,280  z  7,081.    Czas: 0:15:56.\n",
      "  Partia 2,320  z  7,081.    Czas: 0:16:10.\n",
      "  Partia 2,360  z  7,081.    Czas: 0:16:24.\n",
      "  Partia 2,400  z  7,081.    Czas: 0:16:55.\n",
      "  Partia 2,440  z  7,081.    Czas: 0:17:09.\n",
      "  Partia 2,480  z  7,081.    Czas: 0:17:23.\n",
      "  Partia 2,520  z  7,081.    Czas: 0:17:37.\n",
      "  Partia 2,560  z  7,081.    Czas: 0:17:51.\n",
      "  Partia 2,600  z  7,081.    Czas: 0:18:06.\n",
      "  Partia 2,640  z  7,081.    Czas: 0:18:38.\n",
      "  Partia 2,680  z  7,081.    Czas: 0:18:52.\n",
      "  Partia 2,720  z  7,081.    Czas: 0:19:06.\n",
      "  Partia 2,760  z  7,081.    Czas: 0:19:19.\n",
      "  Partia 2,800  z  7,081.    Czas: 0:19:33.\n",
      "  Partia 2,840  z  7,081.    Czas: 0:19:47.\n",
      "  Partia 2,880  z  7,081.    Czas: 0:20:20.\n",
      "  Partia 2,920  z  7,081.    Czas: 0:20:33.\n",
      "  Partia 2,960  z  7,081.    Czas: 0:20:47.\n",
      "  Partia 3,000  z  7,081.    Czas: 0:21:01.\n",
      "  Partia 3,040  z  7,081.    Czas: 0:21:15.\n",
      "  Partia 3,080  z  7,081.    Czas: 0:21:38.\n",
      "  Partia 3,120  z  7,081.    Czas: 0:22:01.\n",
      "  Partia 3,160  z  7,081.    Czas: 0:22:14.\n",
      "  Partia 3,200  z  7,081.    Czas: 0:22:30.\n",
      "  Partia 3,240  z  7,081.    Czas: 0:22:47.\n",
      "  Partia 3,280  z  7,081.    Czas: 0:23:20.\n",
      "  Partia 3,320  z  7,081.    Czas: 0:23:32.\n",
      "  Partia 3,360  z  7,081.    Czas: 0:23:45.\n",
      "  Partia 3,400  z  7,081.    Czas: 0:23:58.\n",
      "  Partia 3,440  z  7,081.    Czas: 0:24:12.\n",
      "  Partia 3,480  z  7,081.    Czas: 0:24:25.\n",
      "  Partia 3,520  z  7,081.    Czas: 0:24:39.\n",
      "  Partia 3,560  z  7,081.    Czas: 0:25:11.\n",
      "  Partia 3,600  z  7,081.    Czas: 0:25:24.\n",
      "  Partia 3,640  z  7,081.    Czas: 0:25:38.\n",
      "  Partia 3,680  z  7,081.    Czas: 0:25:50.\n",
      "  Partia 3,720  z  7,081.    Czas: 0:26:05.\n",
      "  Partia 3,760  z  7,081.    Czas: 0:26:19.\n",
      "  Partia 3,800  z  7,081.    Czas: 0:26:52.\n",
      "  Partia 3,840  z  7,081.    Czas: 0:27:06.\n",
      "  Partia 3,880  z  7,081.    Czas: 0:27:20.\n",
      "  Partia 3,920  z  7,081.    Czas: 0:27:34.\n",
      "  Partia 3,960  z  7,081.    Czas: 0:27:48.\n",
      "  Partia 4,000  z  7,081.    Czas: 0:28:10.\n",
      "  Partia 4,040  z  7,081.    Czas: 0:28:35.\n",
      "  Partia 4,080  z  7,081.    Czas: 0:28:49.\n",
      "  Partia 4,120  z  7,081.    Czas: 0:29:02.\n",
      "  Partia 4,160  z  7,081.    Czas: 0:29:16.\n",
      "  Partia 4,200  z  7,081.    Czas: 0:29:31.\n",
      "  Partia 4,240  z  7,081.    Czas: 0:30:03.\n",
      "  Partia 4,280  z  7,081.    Czas: 0:30:17.\n",
      "  Partia 4,320  z  7,081.    Czas: 0:30:31.\n",
      "  Partia 4,360  z  7,081.    Czas: 0:30:45.\n",
      "  Partia 4,400  z  7,081.    Czas: 0:30:59.\n",
      "  Partia 4,440  z  7,081.    Czas: 0:31:14.\n",
      "  Partia 4,480  z  7,081.    Czas: 0:31:45.\n",
      "  Partia 4,520  z  7,081.    Czas: 0:31:59.\n",
      "  Partia 4,560  z  7,081.    Czas: 0:32:13.\n",
      "  Partia 4,600  z  7,081.    Czas: 0:32:27.\n",
      "  Partia 4,640  z  7,081.    Czas: 0:32:41.\n",
      "  Partia 4,680  z  7,081.    Czas: 0:32:55.\n",
      "  Partia 4,720  z  7,081.    Czas: 0:33:09.\n",
      "  Partia 4,760  z  7,081.    Czas: 0:33:41.\n",
      "  Partia 4,800  z  7,081.    Czas: 0:33:55.\n",
      "  Partia 4,840  z  7,081.    Czas: 0:34:09.\n",
      "  Partia 4,880  z  7,081.    Czas: 0:34:23.\n",
      "  Partia 4,920  z  7,081.    Czas: 0:34:37.\n",
      "  Partia 4,960  z  7,081.    Czas: 0:35:11.\n",
      "  Partia 5,000  z  7,081.    Czas: 0:35:24.\n",
      "  Partia 5,040  z  7,081.    Czas: 0:35:39.\n",
      "  Partia 5,080  z  7,081.    Czas: 0:35:53.\n",
      "  Partia 5,120  z  7,081.    Czas: 0:36:07.\n",
      "  Partia 5,160  z  7,081.    Czas: 0:36:21.\n",
      "  Partia 5,200  z  7,081.    Czas: 0:36:37.\n",
      "  Partia 5,240  z  7,081.    Czas: 0:37:10.\n",
      "  Partia 5,280  z  7,081.    Czas: 0:37:24.\n",
      "  Partia 5,320  z  7,081.    Czas: 0:37:38.\n",
      "  Partia 5,360  z  7,081.    Czas: 0:37:54.\n",
      "  Partia 5,400  z  7,081.    Czas: 0:38:14.\n",
      "  Partia 5,440  z  7,081.    Czas: 0:38:44.\n",
      "  Partia 5,480  z  7,081.    Czas: 0:39:00.\n",
      "  Partia 5,520  z  7,081.    Czas: 0:39:15.\n",
      "  Partia 5,560  z  7,081.    Czas: 0:39:32.\n",
      "  Partia 5,600  z  7,081.    Czas: 0:40:06.\n",
      "  Partia 5,640  z  7,081.    Czas: 0:40:20.\n",
      "  Partia 5,680  z  7,081.    Czas: 0:40:35.\n",
      "  Partia 5,720  z  7,081.    Czas: 0:40:49.\n",
      "  Partia 5,760  z  7,081.    Czas: 0:41:03.\n",
      "  Partia 5,800  z  7,081.    Czas: 0:41:17.\n",
      "  Partia 5,840  z  7,081.    Czas: 0:41:52.\n",
      "  Partia 5,880  z  7,081.    Czas: 0:42:07.\n",
      "  Partia 5,920  z  7,081.    Czas: 0:42:23.\n",
      "  Partia 5,960  z  7,081.    Czas: 0:42:39.\n",
      "  Partia 6,000  z  7,081.    Czas: 0:43:14.\n",
      "  Partia 6,040  z  7,081.    Czas: 0:43:30.\n",
      "  Partia 6,080  z  7,081.    Czas: 0:43:45.\n",
      "  Partia 6,120  z  7,081.    Czas: 0:43:59.\n",
      "  Partia 6,160  z  7,081.    Czas: 0:44:14.\n",
      "  Partia 6,200  z  7,081.    Czas: 0:44:28.\n",
      "  Partia 6,240  z  7,081.    Czas: 0:45:02.\n",
      "  Partia 6,280  z  7,081.    Czas: 0:45:17.\n",
      "  Partia 6,320  z  7,081.    Czas: 0:45:32.\n",
      "  Partia 6,360  z  7,081.    Czas: 0:45:46.\n",
      "  Partia 6,400  z  7,081.    Czas: 0:46:01.\n",
      "  Partia 6,440  z  7,081.    Czas: 0:46:33.\n",
      "  Partia 6,480  z  7,081.    Czas: 0:46:48.\n",
      "  Partia 6,520  z  7,081.    Czas: 0:47:03.\n",
      "  Partia 6,560  z  7,081.    Czas: 0:47:20.\n",
      "  Partia 6,600  z  7,081.    Czas: 0:47:56.\n",
      "  Partia 6,640  z  7,081.    Czas: 0:48:12.\n",
      "  Partia 6,680  z  7,081.    Czas: 0:48:29.\n",
      "  Partia 6,720  z  7,081.    Czas: 0:48:46.\n",
      "  Partia 6,760  z  7,081.    Czas: 0:49:21.\n",
      "  Partia 6,800  z  7,081.    Czas: 0:49:36.\n",
      "  Partia 6,840  z  7,081.    Czas: 0:49:52.\n",
      "  Partia 6,880  z  7,081.    Czas: 0:50:07.\n",
      "  Partia 6,920  z  7,081.    Czas: 0:50:22.\n",
      "  Partia 6,960  z  7,081.    Czas: 0:50:54.\n",
      "  Partia 7,000  z  7,081.    Czas: 0:51:11.\n",
      "  Partia 7,040  z  7,081.    Czas: 0:51:27.\n",
      "  Partia 7,080  z  7,081.    Czas: 0:52:00.\n",
      "\n",
      "  Årednia strata treningu: 0.23\n",
      "  Trening epoki trwaÅ: 0:52:01\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Trening...\n",
      "  Partia    40  z  7,081.    Czas: 0:00:14.\n",
      "  Partia    80  z  7,081.    Czas: 0:00:31.\n",
      "  Partia   120  z  7,081.    Czas: 0:00:46.\n",
      "  Partia   160  z  7,081.    Czas: 0:01:15.\n",
      "  Partia   200  z  7,081.    Czas: 0:01:35.\n",
      "  Partia   240  z  7,081.    Czas: 0:01:49.\n",
      "  Partia   280  z  7,081.    Czas: 0:02:03.\n",
      "  Partia   320  z  7,081.    Czas: 0:02:17.\n",
      "  Partia   360  z  7,081.    Czas: 0:02:33.\n",
      "  Partia   400  z  7,081.    Czas: 0:03:08.\n",
      "  Partia   440  z  7,081.    Czas: 0:03:23.\n",
      "  Partia   480  z  7,081.    Czas: 0:03:37.\n",
      "  Partia   520  z  7,081.    Czas: 0:03:54.\n",
      "  Partia   560  z  7,081.    Czas: 0:04:25.\n",
      "  Partia   600  z  7,081.    Czas: 0:04:43.\n",
      "  Partia   640  z  7,081.    Czas: 0:04:58.\n",
      "  Partia   680  z  7,081.    Czas: 0:05:12.\n",
      "  Partia   720  z  7,081.    Czas: 0:05:27.\n",
      "  Partia   760  z  7,081.    Czas: 0:05:40.\n",
      "  Partia   800  z  7,081.    Czas: 0:06:14.\n",
      "  Partia   840  z  7,081.    Czas: 0:06:28.\n",
      "  Partia   880  z  7,081.    Czas: 0:06:42.\n",
      "  Partia   920  z  7,081.    Czas: 0:06:55.\n",
      "  Partia   960  z  7,081.    Czas: 0:07:09.\n",
      "  Partia 1,000  z  7,081.    Czas: 0:07:24.\n",
      "  Partia 1,040  z  7,081.    Czas: 0:07:56.\n",
      "  Partia 1,080  z  7,081.    Czas: 0:08:11.\n",
      "  Partia 1,120  z  7,081.    Czas: 0:08:24.\n",
      "  Partia 1,160  z  7,081.    Czas: 0:08:38.\n",
      "  Partia 1,200  z  7,081.    Czas: 0:08:53.\n",
      "  Partia 1,240  z  7,081.    Czas: 0:09:07.\n",
      "  Partia 1,280  z  7,081.    Czas: 0:09:37.\n",
      "  Partia 1,320  z  7,081.    Czas: 0:09:51.\n",
      "  Partia 1,360  z  7,081.    Czas: 0:10:05.\n",
      "  Partia 1,400  z  7,081.    Czas: 0:10:19.\n",
      "  Partia 1,440  z  7,081.    Czas: 0:10:33.\n",
      "  Partia 1,480  z  7,081.    Czas: 0:10:47.\n",
      "  Partia 1,520  z  7,081.    Czas: 0:11:19.\n",
      "  Partia 1,560  z  7,081.    Czas: 0:11:33.\n",
      "  Partia 1,600  z  7,081.    Czas: 0:11:47.\n",
      "  Partia 1,640  z  7,081.    Czas: 0:12:01.\n",
      "  Partia 1,680  z  7,081.    Czas: 0:12:16.\n",
      "  Partia 1,720  z  7,081.    Czas: 0:12:31.\n",
      "  Partia 1,760  z  7,081.    Czas: 0:13:05.\n",
      "  Partia 1,800  z  7,081.    Czas: 0:13:19.\n",
      "  Partia 1,840  z  7,081.    Czas: 0:13:33.\n",
      "  Partia 1,880  z  7,081.    Czas: 0:13:47.\n",
      "  Partia 1,920  z  7,081.    Czas: 0:14:02.\n",
      "  Partia 1,960  z  7,081.    Czas: 0:14:34.\n",
      "  Partia 2,000  z  7,081.    Czas: 0:14:48.\n",
      "  Partia 2,040  z  7,081.    Czas: 0:15:02.\n",
      "  Partia 2,080  z  7,081.    Czas: 0:15:15.\n",
      "  Partia 2,120  z  7,081.    Czas: 0:15:29.\n",
      "  Partia 2,160  z  7,081.    Czas: 0:15:43.\n",
      "  Partia 2,200  z  7,081.    Czas: 0:16:15.\n",
      "  Partia 2,240  z  7,081.    Czas: 0:16:30.\n",
      "  Partia 2,280  z  7,081.    Czas: 0:16:43.\n",
      "  Partia 2,320  z  7,081.    Czas: 0:16:57.\n",
      "  Partia 2,360  z  7,081.    Czas: 0:17:12.\n",
      "  Partia 2,400  z  7,081.    Czas: 0:17:47.\n",
      "  Partia 2,440  z  7,081.    Czas: 0:18:02.\n",
      "  Partia 2,480  z  7,081.    Czas: 0:18:16.\n",
      "  Partia 2,520  z  7,081.    Czas: 0:18:30.\n",
      "  Partia 2,560  z  7,081.    Czas: 0:18:43.\n",
      "  Partia 2,600  z  7,081.    Czas: 0:18:57.\n",
      "  Partia 2,640  z  7,081.    Czas: 0:19:30.\n",
      "  Partia 2,680  z  7,081.    Czas: 0:19:44.\n",
      "  Partia 2,720  z  7,081.    Czas: 0:19:58.\n",
      "  Partia 2,760  z  7,081.    Czas: 0:20:12.\n",
      "  Partia 2,800  z  7,081.    Czas: 0:20:26.\n",
      "  Partia 2,840  z  7,081.    Czas: 0:20:45.\n",
      "  Partia 2,880  z  7,081.    Czas: 0:21:15.\n",
      "  Partia 2,920  z  7,081.    Czas: 0:21:29.\n",
      "  Partia 2,960  z  7,081.    Czas: 0:21:43.\n",
      "  Partia 3,000  z  7,081.    Czas: 0:21:58.\n",
      "  Partia 3,040  z  7,081.    Czas: 0:22:29.\n",
      "  Partia 3,080  z  7,081.    Czas: 0:22:43.\n",
      "  Partia 3,120  z  7,081.    Czas: 0:22:58.\n",
      "  Partia 3,160  z  7,081.    Czas: 0:23:12.\n",
      "  Partia 3,200  z  7,081.    Czas: 0:23:27.\n",
      "  Partia 3,240  z  7,081.    Czas: 0:23:41.\n",
      "  Partia 3,280  z  7,081.    Czas: 0:24:15.\n",
      "  Partia 3,320  z  7,081.    Czas: 0:24:29.\n",
      "  Partia 3,360  z  7,081.    Czas: 0:24:44.\n",
      "  Partia 3,400  z  7,081.    Czas: 0:25:00.\n",
      "  Partia 3,440  z  7,081.    Czas: 0:25:35.\n",
      "  Partia 3,480  z  7,081.    Czas: 0:25:51.\n",
      "  Partia 3,520  z  7,081.    Czas: 0:26:06.\n",
      "  Partia 3,560  z  7,081.    Czas: 0:26:21.\n",
      "  Partia 3,600  z  7,081.    Czas: 0:26:39.\n",
      "  Partia 3,640  z  7,081.    Czas: 0:27:09.\n",
      "  Partia 3,680  z  7,081.    Czas: 0:27:23.\n",
      "  Partia 3,720  z  7,081.    Czas: 0:27:38.\n",
      "  Partia 3,760  z  7,081.    Czas: 0:27:53.\n",
      "  Partia 3,800  z  7,081.    Czas: 0:28:28.\n",
      "  Partia 3,840  z  7,081.    Czas: 0:28:43.\n",
      "  Partia 3,880  z  7,081.    Czas: 0:28:57.\n",
      "  Partia 3,920  z  7,081.    Czas: 0:29:15.\n",
      "  Partia 3,960  z  7,081.    Czas: 0:29:33.\n",
      "  Partia 4,000  z  7,081.    Czas: 0:30:10.\n",
      "  Partia 4,040  z  7,081.    Czas: 0:30:30.\n",
      "  Partia 4,080  z  7,081.    Czas: 0:30:47.\n",
      "  Partia 4,120  z  7,081.    Czas: 0:31:01.\n",
      "  Partia 4,160  z  7,081.    Czas: 0:31:24.\n",
      "  Partia 4,200  z  7,081.    Czas: 0:31:49.\n",
      "  Partia 4,240  z  7,081.    Czas: 0:32:04.\n",
      "  Partia 4,280  z  7,081.    Czas: 0:32:17.\n",
      "  Partia 4,320  z  7,081.    Czas: 0:32:29.\n",
      "  Partia 4,360  z  7,081.    Czas: 0:32:42.\n",
      "  Partia 4,400  z  7,081.    Czas: 0:32:55.\n",
      "  Partia 4,440  z  7,081.    Czas: 0:33:07.\n",
      "  Partia 4,480  z  7,081.    Czas: 0:33:21.\n",
      "  Partia 4,520  z  7,081.    Czas: 0:33:52.\n",
      "  Partia 4,560  z  7,081.    Czas: 0:34:05.\n",
      "  Partia 4,600  z  7,081.    Czas: 0:34:19.\n",
      "  Partia 4,640  z  7,081.    Czas: 0:34:34.\n",
      "  Partia 4,680  z  7,081.    Czas: 0:34:48.\n",
      "  Partia 4,720  z  7,081.    Czas: 0:35:14.\n",
      "  Partia 4,760  z  7,081.    Czas: 0:35:36.\n",
      "  Partia 4,800  z  7,081.    Czas: 0:35:51.\n",
      "  Partia 4,840  z  7,081.    Czas: 0:36:07.\n",
      "  Partia 4,880  z  7,081.    Czas: 0:36:22.\n",
      "  Partia 4,920  z  7,081.    Czas: 0:36:56.\n",
      "  Partia 4,960  z  7,081.    Czas: 0:37:10.\n",
      "  Partia 5,000  z  7,081.    Czas: 0:37:24.\n",
      "  Partia 5,040  z  7,081.    Czas: 0:37:39.\n",
      "  Partia 5,080  z  7,081.    Czas: 0:38:13.\n",
      "  Partia 5,120  z  7,081.    Czas: 0:38:28.\n",
      "  Partia 5,160  z  7,081.    Czas: 0:38:42.\n",
      "  Partia 5,200  z  7,081.    Czas: 0:38:57.\n",
      "  Partia 5,240  z  7,081.    Czas: 0:39:11.\n",
      "  Partia 5,280  z  7,081.    Czas: 0:39:44.\n",
      "  Partia 5,320  z  7,081.    Czas: 0:39:58.\n",
      "  Partia 5,360  z  7,081.    Czas: 0:40:15.\n",
      "  Partia 5,400  z  7,081.    Czas: 0:40:29.\n",
      "  Partia 5,440  z  7,081.    Czas: 0:40:42.\n",
      "  Partia 5,480  z  7,081.    Czas: 0:41:15.\n",
      "  Partia 5,520  z  7,081.    Czas: 0:41:29.\n",
      "  Partia 5,560  z  7,081.    Czas: 0:41:43.\n",
      "  Partia 5,600  z  7,081.    Czas: 0:41:58.\n",
      "  Partia 5,640  z  7,081.    Czas: 0:42:32.\n",
      "  Partia 5,680  z  7,081.    Czas: 0:42:46.\n",
      "  Partia 5,720  z  7,081.    Czas: 0:42:59.\n",
      "  Partia 5,760  z  7,081.    Czas: 0:43:13.\n",
      "  Partia 5,800  z  7,081.    Czas: 0:43:27.\n",
      "  Partia 5,840  z  7,081.    Czas: 0:43:50.\n",
      "  Partia 5,880  z  7,081.    Czas: 0:44:14.\n",
      "  Partia 5,920  z  7,081.    Czas: 0:44:32.\n",
      "  Partia 5,960  z  7,081.    Czas: 0:44:47.\n",
      "  Partia 6,000  z  7,081.    Czas: 0:45:01.\n",
      "  Partia 6,040  z  7,081.    Czas: 0:45:19.\n",
      "  Partia 6,080  z  7,081.    Czas: 0:45:50.\n",
      "  Partia 6,120  z  7,081.    Czas: 0:46:04.\n",
      "  Partia 6,160  z  7,081.    Czas: 0:46:18.\n",
      "  Partia 6,200  z  7,081.    Czas: 0:46:32.\n",
      "  Partia 6,240  z  7,081.    Czas: 0:46:46.\n",
      "  Partia 6,280  z  7,081.    Czas: 0:47:04.\n",
      "  Partia 6,320  z  7,081.    Czas: 0:47:32.\n",
      "  Partia 6,360  z  7,081.    Czas: 0:47:46.\n",
      "  Partia 6,400  z  7,081.    Czas: 0:48:00.\n",
      "  Partia 6,440  z  7,081.    Czas: 0:48:14.\n",
      "  Partia 6,480  z  7,081.    Czas: 0:48:28.\n",
      "  Partia 6,520  z  7,081.    Czas: 0:49:02.\n",
      "  Partia 6,560  z  7,081.    Czas: 0:49:16.\n",
      "  Partia 6,600  z  7,081.    Czas: 0:49:30.\n",
      "  Partia 6,640  z  7,081.    Czas: 0:49:44.\n",
      "  Partia 6,680  z  7,081.    Czas: 0:49:58.\n",
      "  Partia 6,720  z  7,081.    Czas: 0:50:12.\n",
      "  Partia 6,760  z  7,081.    Czas: 0:50:46.\n",
      "  Partia 6,800  z  7,081.    Czas: 0:50:59.\n",
      "  Partia 6,840  z  7,081.    Czas: 0:51:14.\n",
      "  Partia 6,880  z  7,081.    Czas: 0:51:28.\n",
      "  Partia 6,920  z  7,081.    Czas: 0:51:42.\n",
      "  Partia 6,960  z  7,081.    Czas: 0:52:01.\n",
      "  Partia 7,000  z  7,081.    Czas: 0:52:31.\n",
      "  Partia 7,040  z  7,081.    Czas: 0:52:45.\n",
      "  Partia 7,080  z  7,081.    Czas: 0:53:00.\n",
      "\n",
      "  Årednia strata treningu: 0.16\n",
      "  Trening epoki trwaÅ: 0:53:00\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Trening...\n",
      "  Partia    40  z  7,081.    Czas: 0:00:14.\n",
      "  Partia    80  z  7,081.    Czas: 0:00:29.\n",
      "  Partia   120  z  7,081.    Czas: 0:01:03.\n",
      "  Partia   160  z  7,081.    Czas: 0:01:17.\n",
      "  Partia   200  z  7,081.    Czas: 0:01:31.\n",
      "  Partia   240  z  7,081.    Czas: 0:01:46.\n",
      "  Partia   280  z  7,081.    Czas: 0:02:00.\n",
      "  Partia   320  z  7,081.    Czas: 0:02:34.\n",
      "  Partia   360  z  7,081.    Czas: 0:02:48.\n",
      "  Partia   400  z  7,081.    Czas: 0:03:03.\n",
      "  Partia   440  z  7,081.    Czas: 0:03:17.\n",
      "  Partia   480  z  7,081.    Czas: 0:03:32.\n",
      "  Partia   520  z  7,081.    Czas: 0:03:58.\n",
      "  Partia   560  z  7,081.    Czas: 0:04:20.\n",
      "  Partia   600  z  7,081.    Czas: 0:04:34.\n",
      "  Partia   640  z  7,081.    Czas: 0:04:48.\n",
      "  Partia   680  z  7,081.    Czas: 0:05:03.\n",
      "  Partia   720  z  7,081.    Czas: 0:05:17.\n",
      "  Partia   760  z  7,081.    Czas: 0:05:49.\n",
      "  Partia   800  z  7,081.    Czas: 0:06:03.\n",
      "  Partia   840  z  7,081.    Czas: 0:06:17.\n",
      "  Partia   880  z  7,081.    Czas: 0:06:31.\n",
      "  Partia   920  z  7,081.    Czas: 0:06:46.\n",
      "  Partia   960  z  7,081.    Czas: 0:07:14.\n",
      "  Partia 1,000  z  7,081.    Czas: 0:07:33.\n",
      "  Partia 1,040  z  7,081.    Czas: 0:07:47.\n",
      "  Partia 1,080  z  7,081.    Czas: 0:08:01.\n",
      "  Partia 1,120  z  7,081.    Czas: 0:08:15.\n",
      "  Partia 1,160  z  7,081.    Czas: 0:08:30.\n",
      "  Partia 1,200  z  7,081.    Czas: 0:09:03.\n",
      "  Partia 1,240  z  7,081.    Czas: 0:09:17.\n",
      "  Partia 1,280  z  7,081.    Czas: 0:09:32.\n",
      "  Partia 1,320  z  7,081.    Czas: 0:09:46.\n",
      "  Partia 1,360  z  7,081.    Czas: 0:10:19.\n",
      "  Partia 1,400  z  7,081.    Czas: 0:10:32.\n",
      "  Partia 1,440  z  7,081.    Czas: 0:10:46.\n",
      "  Partia 1,480  z  7,081.    Czas: 0:11:01.\n",
      "  Partia 1,520  z  7,081.    Czas: 0:11:16.\n",
      "  Partia 1,560  z  7,081.    Czas: 0:11:50.\n",
      "  Partia 1,600  z  7,081.    Czas: 0:12:04.\n",
      "  Partia 1,640  z  7,081.    Czas: 0:12:18.\n",
      "  Partia 1,680  z  7,081.    Czas: 0:12:33.\n",
      "  Partia 1,720  z  7,081.    Czas: 0:12:46.\n",
      "  Partia 1,760  z  7,081.    Czas: 0:13:00.\n",
      "  Partia 1,800  z  7,081.    Czas: 0:13:32.\n",
      "  Partia 1,840  z  7,081.    Czas: 0:13:46.\n",
      "  Partia 1,880  z  7,081.    Czas: 0:13:59.\n",
      "  Partia 1,920  z  7,081.    Czas: 0:14:13.\n",
      "  Partia 1,960  z  7,081.    Czas: 0:14:27.\n",
      "  Partia 2,000  z  7,081.    Czas: 0:14:56.\n",
      "  Partia 2,040  z  7,081.    Czas: 0:15:17.\n",
      "  Partia 2,080  z  7,081.    Czas: 0:15:32.\n",
      "  Partia 2,120  z  7,081.    Czas: 0:15:47.\n",
      "  Partia 2,160  z  7,081.    Czas: 0:16:01.\n",
      "  Partia 2,200  z  7,081.    Czas: 0:16:16.\n",
      "  Partia 2,240  z  7,081.    Czas: 0:16:50.\n",
      "  Partia 2,280  z  7,081.    Czas: 0:17:04.\n",
      "  Partia 2,320  z  7,081.    Czas: 0:17:18.\n",
      "  Partia 2,360  z  7,081.    Czas: 0:17:32.\n",
      "  Partia 2,400  z  7,081.    Czas: 0:17:46.\n",
      "  Partia 2,440  z  7,081.    Czas: 0:18:00.\n",
      "  Partia 2,480  z  7,081.    Czas: 0:18:35.\n",
      "  Partia 2,520  z  7,081.    Czas: 0:18:49.\n",
      "  Partia 2,560  z  7,081.    Czas: 0:19:04.\n",
      "  Partia 2,600  z  7,081.    Czas: 0:19:18.\n",
      "  Partia 2,640  z  7,081.    Czas: 0:19:32.\n",
      "  Partia 2,680  z  7,081.    Czas: 0:19:54.\n",
      "  Partia 2,720  z  7,081.    Czas: 0:20:20.\n",
      "  Partia 2,760  z  7,081.    Czas: 0:20:34.\n",
      "  Partia 2,800  z  7,081.    Czas: 0:20:49.\n",
      "  Partia 2,840  z  7,081.    Czas: 0:21:03.\n",
      "  Partia 2,880  z  7,081.    Czas: 0:21:35.\n",
      "  Partia 2,920  z  7,081.    Czas: 0:21:50.\n",
      "  Partia 2,960  z  7,081.    Czas: 0:22:04.\n",
      "  Partia 3,000  z  7,081.    Czas: 0:22:20.\n",
      "  Partia 3,040  z  7,081.    Czas: 0:22:53.\n",
      "  Partia 3,080  z  7,081.    Czas: 0:23:07.\n",
      "  Partia 3,120  z  7,081.    Czas: 0:23:22.\n",
      "  Partia 3,160  z  7,081.    Czas: 0:23:36.\n",
      "  Partia 3,200  z  7,081.    Czas: 0:23:50.\n",
      "  Partia 3,240  z  7,081.    Czas: 0:24:24.\n",
      "  Partia 3,280  z  7,081.    Czas: 0:24:39.\n",
      "  Partia 3,320  z  7,081.    Czas: 0:24:54.\n",
      "  Partia 3,360  z  7,081.    Czas: 0:25:08.\n",
      "  Partia 3,400  z  7,081.    Czas: 0:25:22.\n",
      "  Partia 3,440  z  7,081.    Czas: 0:25:39.\n",
      "  Partia 3,480  z  7,081.    Czas: 0:26:10.\n",
      "  Partia 3,520  z  7,081.    Czas: 0:26:23.\n",
      "  Partia 3,560  z  7,081.    Czas: 0:26:37.\n",
      "  Partia 3,600  z  7,081.    Czas: 0:26:51.\n",
      "  Partia 3,640  z  7,081.    Czas: 0:27:05.\n",
      "  Partia 3,680  z  7,081.    Czas: 0:27:39.\n",
      "  Partia 3,720  z  7,081.    Czas: 0:27:54.\n",
      "  Partia 3,760  z  7,081.    Czas: 0:28:10.\n",
      "  Partia 3,800  z  7,081.    Czas: 0:28:26.\n",
      "  Partia 3,840  z  7,081.    Czas: 0:28:41.\n",
      "  Partia 3,880  z  7,081.    Czas: 0:29:18.\n",
      "  Partia 3,920  z  7,081.    Czas: 0:29:34.\n",
      "  Partia 3,960  z  7,081.    Czas: 0:29:49.\n",
      "  Partia 4,000  z  7,081.    Czas: 0:30:05.\n",
      "  Partia 4,040  z  7,081.    Czas: 0:30:40.\n",
      "  Partia 4,080  z  7,081.    Czas: 0:30:55.\n",
      "  Partia 4,120  z  7,081.    Czas: 0:31:11.\n",
      "  Partia 4,160  z  7,081.    Czas: 0:31:26.\n",
      "  Partia 4,200  z  7,081.    Czas: 0:31:41.\n",
      "  Partia 4,240  z  7,081.    Czas: 0:32:16.\n",
      "  Partia 4,280  z  7,081.    Czas: 0:32:30.\n",
      "  Partia 4,320  z  7,081.    Czas: 0:32:44.\n",
      "  Partia 4,360  z  7,081.    Czas: 0:32:58.\n",
      "  Partia 4,400  z  7,081.    Czas: 0:33:12.\n",
      "  Partia 4,440  z  7,081.    Czas: 0:33:45.\n",
      "  Partia 4,480  z  7,081.    Czas: 0:33:59.\n",
      "  Partia 4,520  z  7,081.    Czas: 0:34:13.\n",
      "  Partia 4,560  z  7,081.    Czas: 0:34:27.\n",
      "  Partia 4,600  z  7,081.    Czas: 0:34:41.\n",
      "  Partia 4,640  z  7,081.    Czas: 0:35:13.\n",
      "  Partia 4,680  z  7,081.    Czas: 0:35:27.\n",
      "  Partia 4,720  z  7,081.    Czas: 0:35:42.\n",
      "  Partia 4,760  z  7,081.    Czas: 0:35:56.\n",
      "  Partia 4,800  z  7,081.    Czas: 0:36:10.\n",
      "  Partia 4,840  z  7,081.    Czas: 0:36:24.\n",
      "  Partia 4,880  z  7,081.    Czas: 0:36:56.\n",
      "  Partia 4,920  z  7,081.    Czas: 0:37:10.\n",
      "  Partia 4,960  z  7,081.    Czas: 0:37:24.\n",
      "  Partia 5,000  z  7,081.    Czas: 0:37:38.\n",
      "  Partia 5,040  z  7,081.    Czas: 0:37:52.\n",
      "  Partia 5,080  z  7,081.    Czas: 0:38:23.\n",
      "  Partia 5,120  z  7,081.    Czas: 0:38:40.\n",
      "  Partia 5,160  z  7,081.    Czas: 0:38:54.\n",
      "  Partia 5,200  z  7,081.    Czas: 0:39:08.\n",
      "  Partia 5,240  z  7,081.    Czas: 0:39:22.\n",
      "  Partia 5,280  z  7,081.    Czas: 0:39:36.\n",
      "  Partia 5,320  z  7,081.    Czas: 0:40:10.\n",
      "  Partia 5,360  z  7,081.    Czas: 0:40:24.\n",
      "  Partia 5,400  z  7,081.    Czas: 0:40:38.\n",
      "  Partia 5,440  z  7,081.    Czas: 0:40:52.\n",
      "  Partia 5,480  z  7,081.    Czas: 0:41:07.\n",
      "  Partia 5,520  z  7,081.    Czas: 0:41:39.\n",
      "  Partia 5,560  z  7,081.    Czas: 0:41:53.\n",
      "  Partia 5,600  z  7,081.    Czas: 0:42:07.\n",
      "  Partia 5,640  z  7,081.    Czas: 0:42:21.\n",
      "  Partia 5,680  z  7,081.    Czas: 0:42:35.\n",
      "  Partia 5,720  z  7,081.    Czas: 0:42:53.\n",
      "  Partia 5,760  z  7,081.    Czas: 0:43:22.\n",
      "  Partia 5,800  z  7,081.    Czas: 0:43:37.\n",
      "  Partia 5,840  z  7,081.    Czas: 0:43:52.\n",
      "  Partia 5,880  z  7,081.    Czas: 0:44:11.\n",
      "  Partia 5,920  z  7,081.    Czas: 0:44:40.\n",
      "  Partia 5,960  z  7,081.    Czas: 0:44:55.\n",
      "  Partia 6,000  z  7,081.    Czas: 0:45:09.\n",
      "  Partia 6,040  z  7,081.    Czas: 0:45:23.\n",
      "  Partia 6,080  z  7,081.    Czas: 0:45:37.\n",
      "  Partia 6,120  z  7,081.    Czas: 0:45:54.\n",
      "  Partia 6,160  z  7,081.    Czas: 0:46:24.\n",
      "  Partia 6,200  z  7,081.    Czas: 0:46:39.\n",
      "  Partia 6,240  z  7,081.    Czas: 0:46:53.\n",
      "  Partia 6,280  z  7,081.    Czas: 0:47:07.\n",
      "  Partia 6,320  z  7,081.    Czas: 0:47:21.\n",
      "  Partia 6,360  z  7,081.    Czas: 0:47:41.\n",
      "  Partia 6,400  z  7,081.    Czas: 0:48:10.\n",
      "  Partia 6,440  z  7,081.    Czas: 0:48:24.\n",
      "  Partia 6,480  z  7,081.    Czas: 0:48:37.\n",
      "  Partia 6,520  z  7,081.    Czas: 0:48:50.\n",
      "  Partia 6,560  z  7,081.    Czas: 0:49:03.\n",
      "  Partia 6,600  z  7,081.    Czas: 0:49:35.\n",
      "  Partia 6,640  z  7,081.    Czas: 0:49:48.\n",
      "  Partia 6,680  z  7,081.    Czas: 0:50:01.\n",
      "  Partia 6,720  z  7,081.    Czas: 0:50:14.\n",
      "  Partia 6,760  z  7,081.    Czas: 0:50:27.\n",
      "  Partia 6,800  z  7,081.    Czas: 0:50:40.\n",
      "  Partia 6,840  z  7,081.    Czas: 0:50:53.\n",
      "  Partia 6,880  z  7,081.    Czas: 0:51:07.\n",
      "  Partia 6,920  z  7,081.    Czas: 0:51:36.\n",
      "  Partia 6,960  z  7,081.    Czas: 0:51:49.\n",
      "  Partia 7,000  z  7,081.    Czas: 0:52:03.\n",
      "  Partia 7,040  z  7,081.    Czas: 0:52:17.\n",
      "  Partia 7,080  z  7,081.    Czas: 0:52:51.\n",
      "\n",
      "  Årednia strata treningu: 0.11\n",
      "  Trening epoki trwaÅ: 0:52:51\n",
      "\n",
      "Trening zakoÅczony!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # zalecane learning rate dla AdamW\n",
    "                  eps = 1e-8 # zalecany epsilon dla AdamW\n",
    "                )\n",
    "\n",
    "#AdamW is a certain optimizer very popular in NLP models using transformers, it uses the mechanism\n",
    "#\"weight decay\", which is intended to prevent the model from overtraining\n",
    "#lr- specifies the speed at which the model changes its weights in response to an error\n",
    "#pes - is intended to prevent us from dividing by zero\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "#(epoch) specifies the number of iterations over the entire training data set\n",
    "epochs=4\n",
    "\n",
    "# The total number of training steps is [number of batches] x [number of epochs].\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create a learning schedule with warmup.\n",
    "#A training schedule with a warm-up period allows the model to start at a lower learning rate\n",
    "#and gradually increase to a defined rate, helping to stabilize the initial phases of training.\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "\n",
    "# Function to calculate prediction accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Function to format time as hh:mm:ss\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Grain setting for repeatability\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Storing average loss per batch\n",
    "loss_values = []\n",
    "\n",
    "# Loop through each era\n",
    "\n",
    "#during the training section we are:\n",
    "#making gradient zeroing: This prevents the accumulation of gradients from previous steps, which is important for the training process to work properly.\n",
    "#calculating the los: by this we mean that every batch is processed by the model, which returns the loss for this batch.\n",
    "#Backpropagation (loss.backward()): The backpropagation algorithm calculates the gradient of the loss function with respect to the model weights.\n",
    "#Gradient clipping (clip_grad_norm_): This limits the size of gradients to a maximum norm of 1.0\n",
    "#updating the weights of the model\n",
    "#Learning schedule update: After each step, the learning schedule parameters are updated.\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Trenowanie\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Trening...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Partia {:>5,}  z  {:>5,}.    Czas: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Limit gradients to 1.0 to prevent them from growing too large\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        \n",
    "        # Update parameters and step in learning schedule\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculation of average loss over batches\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Recording the loss of this era\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Årednia strata treningu: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Trening epoki trwaÅ: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "print(\"\")\n",
    "print(\"Trening zakoÅczony!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in this section below we are performing the process of the evaluation of <span style=\"color:lightblue\">BERT</span> model that has been<br>\n",
    "previosly trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.91      0.93      0.92     82343\n",
      "     Class 1       0.76      0.71      0.73     25554\n",
      "\n",
      "    accuracy                           0.88    107897\n",
      "   macro avg       0.83      0.82      0.83    107897\n",
      "weighted avg       0.87      0.88      0.88    107897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Preparing containers for forecasts and real labels\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "# Computing device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Iterate over the validation DataLoader\n",
    "for batch in validation_dataloader:\n",
    "    \n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "  \n",
    "    # Transfer predictions and true labels to the CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "    # Save predictions and labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Select the class with the higher logit value as the forecast\n",
    "preds = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate model evaluation metrics for each clas\n",
    "class_report = classification_report(true_labels, preds, target_names=['Class 0', 'Class 1'])\n",
    "\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer with balanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- here we will not explain every line or some concepts as we did it in the section <span style=\"color:green\">Transformer with not balanced data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- here we are preprocessing and preparing data set for the use with BERT(Bidirectional Encoder Representations from Transformers) model, <br>\n",
    "we are using `bert-base-uncased`. <span style=\"color:lightblue\">BERT</span> is a advanced language model developed by Google in 2018. <br>\n",
    "It is one form of a transformer which are used to understand and generate human language text. This model is<br>\n",
    "is provided petrained and what we do is <bold>fine-tuning</bold>. In this phase BERT is trained on much smaller data set, which is directly related to the<br>\n",
    "task for which the model is to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomUnderSampler object\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "tweets_df_without_target = tweets_df.drop('Target', axis=1)\n",
    "dev_val_df_without_target= dev_val_df.drop('Target', axis=1)\n",
    "\n",
    "# Transform data using undersampling\n",
    "X_train_transformer_balanced, y_train_transformer_balanced = rus.fit_resample(tweets_df_without_target, tweets_df['Target'])\n",
    "\n",
    "X_test_transformer_balanced, y_test_transformer_balanced = rus.fit_resample(dev_val_df_without_target, dev_val_df['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\igorr\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for tweet in X_train_transformer_balanced['Text']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tweet,\n",
    "                        add_special_tokens = True,  \n",
    "                        max_length = 64,           \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',     \n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(y_train_transformer_balanced.values)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input_ids = []\n",
    "new_attention_masks = []\n",
    "\n",
    "for tweet in X_test_transformer_balanced['Text']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tweet,\n",
    "                        add_special_tokens = True,  \n",
    "                        max_length = 64,           \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',    \n",
    "                   )\n",
    "    \n",
    "    new_input_ids.append(encoded_dict['input_ids'])\n",
    "    new_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "new_input_ids = torch.cat(new_input_ids, dim=0)\n",
    "new_attention_masks = torch.cat(new_attention_masks, dim=0)\n",
    "new_labels = torch.tensor(y_test_transformer_balanced.values)\n",
    "\n",
    "new_val_dataset = TensorDataset(new_input_ids, new_attention_masks, new_labels)\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            new_val_dataset,\n",
    "            sampler = SequentialSampler(new_val_dataset),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels = 2,  \n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\igorr\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  3,357.    Elapsed: 0:00:14.\n",
      "  Batch    80  of  3,357.    Elapsed: 0:00:24.\n",
      "  Batch   120  of  3,357.    Elapsed: 0:00:32.\n",
      "  Batch   160  of  3,357.    Elapsed: 0:00:41.\n",
      "  Batch   200  of  3,357.    Elapsed: 0:00:50.\n",
      "  Batch   240  of  3,357.    Elapsed: 0:00:59.\n",
      "  Batch   280  of  3,357.    Elapsed: 0:01:08.\n",
      "  Batch   320  of  3,357.    Elapsed: 0:01:16.\n",
      "  Batch   360  of  3,357.    Elapsed: 0:01:25.\n",
      "  Batch   400  of  3,357.    Elapsed: 0:01:34.\n",
      "  Batch   440  of  3,357.    Elapsed: 0:01:43.\n",
      "  Batch   480  of  3,357.    Elapsed: 0:01:52.\n",
      "  Batch   520  of  3,357.    Elapsed: 0:02:01.\n",
      "  Batch   560  of  3,357.    Elapsed: 0:02:10.\n",
      "  Batch   600  of  3,357.    Elapsed: 0:02:19.\n",
      "  Batch   640  of  3,357.    Elapsed: 0:02:28.\n",
      "  Batch   680  of  3,357.    Elapsed: 0:02:37.\n",
      "  Batch   720  of  3,357.    Elapsed: 0:02:46.\n",
      "  Batch   760  of  3,357.    Elapsed: 0:02:55.\n",
      "  Batch   800  of  3,357.    Elapsed: 0:03:05.\n",
      "  Batch   840  of  3,357.    Elapsed: 0:03:14.\n",
      "  Batch   880  of  3,357.    Elapsed: 0:03:23.\n",
      "  Batch   920  of  3,357.    Elapsed: 0:03:32.\n",
      "  Batch   960  of  3,357.    Elapsed: 0:03:41.\n",
      "  Batch 1,000  of  3,357.    Elapsed: 0:03:51.\n",
      "  Batch 1,040  of  3,357.    Elapsed: 0:04:11.\n",
      "  Batch 1,080  of  3,357.    Elapsed: 0:04:25.\n",
      "  Batch 1,120  of  3,357.    Elapsed: 0:04:34.\n",
      "  Batch 1,160  of  3,357.    Elapsed: 0:04:44.\n",
      "  Batch 1,200  of  3,357.    Elapsed: 0:04:53.\n",
      "  Batch 1,240  of  3,357.    Elapsed: 0:05:02.\n",
      "  Batch 1,280  of  3,357.    Elapsed: 0:05:11.\n",
      "  Batch 1,320  of  3,357.    Elapsed: 0:05:21.\n",
      "  Batch 1,360  of  3,357.    Elapsed: 0:05:30.\n",
      "  Batch 1,400  of  3,357.    Elapsed: 0:05:39.\n",
      "  Batch 1,440  of  3,357.    Elapsed: 0:05:49.\n",
      "  Batch 1,480  of  3,357.    Elapsed: 0:05:58.\n",
      "  Batch 1,520  of  3,357.    Elapsed: 0:06:13.\n",
      "  Batch 1,560  of  3,357.    Elapsed: 0:06:32.\n",
      "  Batch 1,600  of  3,357.    Elapsed: 0:06:42.\n",
      "  Batch 1,640  of  3,357.    Elapsed: 0:06:51.\n",
      "  Batch 1,680  of  3,357.    Elapsed: 0:07:00.\n",
      "  Batch 1,720  of  3,357.    Elapsed: 0:07:10.\n",
      "  Batch 1,760  of  3,357.    Elapsed: 0:07:19.\n",
      "  Batch 1,800  of  3,357.    Elapsed: 0:07:28.\n",
      "  Batch 1,840  of  3,357.    Elapsed: 0:07:37.\n",
      "  Batch 1,880  of  3,357.    Elapsed: 0:08:03.\n",
      "  Batch 1,920  of  3,357.    Elapsed: 0:08:13.\n",
      "  Batch 1,960  of  3,357.    Elapsed: 0:08:22.\n",
      "  Batch 2,000  of  3,357.    Elapsed: 0:08:31.\n",
      "  Batch 2,040  of  3,357.    Elapsed: 0:08:40.\n",
      "  Batch 2,080  of  3,357.    Elapsed: 0:08:50.\n",
      "  Batch 2,120  of  3,357.    Elapsed: 0:08:59.\n",
      "  Batch 2,160  of  3,357.    Elapsed: 0:09:08.\n",
      "  Batch 2,200  of  3,357.    Elapsed: 0:09:18.\n",
      "  Batch 2,240  of  3,357.    Elapsed: 0:09:27.\n",
      "  Batch 2,280  of  3,357.    Elapsed: 0:09:52.\n",
      "  Batch 2,320  of  3,357.    Elapsed: 0:10:01.\n",
      "  Batch 2,360  of  3,357.    Elapsed: 0:10:10.\n",
      "  Batch 2,400  of  3,357.    Elapsed: 0:10:20.\n",
      "  Batch 2,440  of  3,357.    Elapsed: 0:10:29.\n",
      "  Batch 2,480  of  3,357.    Elapsed: 0:10:38.\n",
      "  Batch 2,520  of  3,357.    Elapsed: 0:10:48.\n",
      "  Batch 2,560  of  3,357.    Elapsed: 0:10:58.\n",
      "  Batch 2,600  of  3,357.    Elapsed: 0:11:15.\n",
      "  Batch 2,640  of  3,357.    Elapsed: 0:11:35.\n",
      "  Batch 2,680  of  3,357.    Elapsed: 0:11:44.\n",
      "  Batch 2,720  of  3,357.    Elapsed: 0:11:53.\n",
      "  Batch 2,760  of  3,357.    Elapsed: 0:12:02.\n",
      "  Batch 2,800  of  3,357.    Elapsed: 0:12:12.\n",
      "  Batch 2,840  of  3,357.    Elapsed: 0:12:21.\n",
      "  Batch 2,880  of  3,357.    Elapsed: 0:12:30.\n",
      "  Batch 2,920  of  3,357.    Elapsed: 0:12:39.\n",
      "  Batch 2,960  of  3,357.    Elapsed: 0:12:59.\n",
      "  Batch 3,000  of  3,357.    Elapsed: 0:13:15.\n",
      "  Batch 3,040  of  3,357.    Elapsed: 0:13:24.\n",
      "  Batch 3,080  of  3,357.    Elapsed: 0:13:33.\n",
      "  Batch 3,120  of  3,357.    Elapsed: 0:13:43.\n",
      "  Batch 3,160  of  3,357.    Elapsed: 0:13:52.\n",
      "  Batch 3,200  of  3,357.    Elapsed: 0:14:01.\n",
      "  Batch 3,240  of  3,357.    Elapsed: 0:14:11.\n",
      "  Batch 3,280  of  3,357.    Elapsed: 0:14:26.\n",
      "  Batch 3,320  of  3,357.    Elapsed: 0:14:46.\n",
      "\n",
      "  Average training loss: 0.40\n",
      "  Training epoch took: 0:14:54\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  3,357.    Elapsed: 0:00:09.\n",
      "  Batch    80  of  3,357.    Elapsed: 0:00:19.\n",
      "  Batch   120  of  3,357.    Elapsed: 0:00:28.\n",
      "  Batch   160  of  3,357.    Elapsed: 0:00:37.\n",
      "  Batch   200  of  3,357.    Elapsed: 0:00:46.\n",
      "  Batch   240  of  3,357.    Elapsed: 0:00:56.\n",
      "  Batch   280  of  3,357.    Elapsed: 0:01:21.\n",
      "  Batch   320  of  3,357.    Elapsed: 0:01:30.\n",
      "  Batch   360  of  3,357.    Elapsed: 0:01:40.\n",
      "  Batch   400  of  3,357.    Elapsed: 0:01:49.\n",
      "  Batch   440  of  3,357.    Elapsed: 0:01:58.\n",
      "  Batch   480  of  3,357.    Elapsed: 0:02:07.\n",
      "  Batch   520  of  3,357.    Elapsed: 0:02:16.\n",
      "  Batch   560  of  3,357.    Elapsed: 0:02:26.\n",
      "  Batch   600  of  3,357.    Elapsed: 0:02:35.\n",
      "  Batch   640  of  3,357.    Elapsed: 0:03:01.\n",
      "  Batch   680  of  3,357.    Elapsed: 0:03:11.\n",
      "  Batch   720  of  3,357.    Elapsed: 0:03:20.\n",
      "  Batch   760  of  3,357.    Elapsed: 0:03:29.\n",
      "  Batch   800  of  3,357.    Elapsed: 0:03:38.\n",
      "  Batch   840  of  3,357.    Elapsed: 0:03:47.\n",
      "  Batch   880  of  3,357.    Elapsed: 0:03:57.\n",
      "  Batch   920  of  3,357.    Elapsed: 0:04:06.\n",
      "  Batch   960  of  3,357.    Elapsed: 0:04:32.\n",
      "  Batch 1,000  of  3,357.    Elapsed: 0:04:42.\n",
      "  Batch 1,040  of  3,357.    Elapsed: 0:04:51.\n",
      "  Batch 1,080  of  3,357.    Elapsed: 0:05:00.\n",
      "  Batch 1,120  of  3,357.    Elapsed: 0:05:09.\n",
      "  Batch 1,160  of  3,357.    Elapsed: 0:05:19.\n",
      "  Batch 1,200  of  3,357.    Elapsed: 0:05:28.\n",
      "  Batch 1,240  of  3,357.    Elapsed: 0:05:53.\n",
      "  Batch 1,280  of  3,357.    Elapsed: 0:06:02.\n",
      "  Batch 1,320  of  3,357.    Elapsed: 0:06:12.\n",
      "  Batch 1,360  of  3,357.    Elapsed: 0:06:21.\n",
      "  Batch 1,400  of  3,357.    Elapsed: 0:06:30.\n",
      "  Batch 1,440  of  3,357.    Elapsed: 0:06:39.\n",
      "  Batch 1,480  of  3,357.    Elapsed: 0:06:49.\n",
      "  Batch 1,520  of  3,357.    Elapsed: 0:06:58.\n",
      "  Batch 1,560  of  3,357.    Elapsed: 0:07:23.\n",
      "  Batch 1,600  of  3,357.    Elapsed: 0:07:32.\n",
      "  Batch 1,640  of  3,357.    Elapsed: 0:07:42.\n",
      "  Batch 1,680  of  3,357.    Elapsed: 0:07:51.\n",
      "  Batch 1,720  of  3,357.    Elapsed: 0:08:00.\n",
      "  Batch 1,760  of  3,357.    Elapsed: 0:08:09.\n",
      "  Batch 1,800  of  3,357.    Elapsed: 0:08:18.\n",
      "  Batch 1,840  of  3,357.    Elapsed: 0:08:28.\n",
      "  Batch 1,880  of  3,357.    Elapsed: 0:08:46.\n",
      "  Batch 1,920  of  3,357.    Elapsed: 0:09:03.\n",
      "  Batch 1,960  of  3,357.    Elapsed: 0:09:13.\n",
      "  Batch 2,000  of  3,357.    Elapsed: 0:09:22.\n",
      "  Batch 2,040  of  3,357.    Elapsed: 0:09:31.\n",
      "  Batch 2,080  of  3,357.    Elapsed: 0:09:40.\n",
      "  Batch 2,120  of  3,357.    Elapsed: 0:09:49.\n",
      "  Batch 2,160  of  3,357.    Elapsed: 0:09:59.\n",
      "  Batch 2,200  of  3,357.    Elapsed: 0:10:14.\n",
      "  Batch 2,240  of  3,357.    Elapsed: 0:10:34.\n",
      "  Batch 2,280  of  3,357.    Elapsed: 0:10:43.\n",
      "  Batch 2,320  of  3,357.    Elapsed: 0:10:52.\n",
      "  Batch 2,360  of  3,357.    Elapsed: 0:11:01.\n",
      "  Batch 2,400  of  3,357.    Elapsed: 0:11:11.\n",
      "  Batch 2,440  of  3,357.    Elapsed: 0:11:20.\n",
      "  Batch 2,480  of  3,357.    Elapsed: 0:11:29.\n",
      "  Batch 2,520  of  3,357.    Elapsed: 0:11:49.\n",
      "  Batch 2,560  of  3,357.    Elapsed: 0:12:05.\n",
      "  Batch 2,600  of  3,357.    Elapsed: 0:12:14.\n",
      "  Batch 2,640  of  3,357.    Elapsed: 0:12:23.\n",
      "  Batch 2,680  of  3,357.    Elapsed: 0:12:32.\n",
      "  Batch 2,720  of  3,357.    Elapsed: 0:12:41.\n",
      "  Batch 2,760  of  3,357.    Elapsed: 0:12:51.\n",
      "  Batch 2,800  of  3,357.    Elapsed: 0:13:00.\n",
      "  Batch 2,840  of  3,357.    Elapsed: 0:13:09.\n",
      "  Batch 2,880  of  3,357.    Elapsed: 0:13:36.\n",
      "  Batch 2,920  of  3,357.    Elapsed: 0:13:45.\n",
      "  Batch 2,960  of  3,357.    Elapsed: 0:13:55.\n",
      "  Batch 3,000  of  3,357.    Elapsed: 0:14:04.\n",
      "  Batch 3,040  of  3,357.    Elapsed: 0:14:13.\n",
      "  Batch 3,080  of  3,357.    Elapsed: 0:14:22.\n",
      "  Batch 3,120  of  3,357.    Elapsed: 0:14:32.\n",
      "  Batch 3,160  of  3,357.    Elapsed: 0:14:41.\n",
      "  Batch 3,200  of  3,357.    Elapsed: 0:15:07.\n",
      "  Batch 3,240  of  3,357.    Elapsed: 0:15:16.\n",
      "  Batch 3,280  of  3,357.    Elapsed: 0:15:25.\n",
      "  Batch 3,320  of  3,357.    Elapsed: 0:15:35.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:15:43\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  3,357.    Elapsed: 0:00:09.\n",
      "  Batch    80  of  3,357.    Elapsed: 0:00:18.\n",
      "  Batch   120  of  3,357.    Elapsed: 0:00:28.\n",
      "  Batch   160  of  3,357.    Elapsed: 0:00:37.\n",
      "  Batch   200  of  3,357.    Elapsed: 0:01:02.\n",
      "  Batch   240  of  3,357.    Elapsed: 0:01:12.\n",
      "  Batch   280  of  3,357.    Elapsed: 0:01:21.\n",
      "  Batch   320  of  3,357.    Elapsed: 0:01:30.\n",
      "  Batch   360  of  3,357.    Elapsed: 0:01:39.\n",
      "  Batch   400  of  3,357.    Elapsed: 0:01:49.\n",
      "  Batch   440  of  3,357.    Elapsed: 0:01:58.\n",
      "  Batch   480  of  3,357.    Elapsed: 0:02:11.\n",
      "  Batch   520  of  3,357.    Elapsed: 0:02:33.\n",
      "  Batch   560  of  3,357.    Elapsed: 0:02:42.\n",
      "  Batch   600  of  3,357.    Elapsed: 0:02:51.\n",
      "  Batch   640  of  3,357.    Elapsed: 0:03:00.\n",
      "  Batch   680  of  3,357.    Elapsed: 0:03:10.\n",
      "  Batch   720  of  3,357.    Elapsed: 0:03:19.\n",
      "  Batch   760  of  3,357.    Elapsed: 0:03:28.\n",
      "  Batch   800  of  3,357.    Elapsed: 0:03:38.\n",
      "  Batch   840  of  3,357.    Elapsed: 0:04:03.\n",
      "  Batch   880  of  3,357.    Elapsed: 0:04:12.\n",
      "  Batch   920  of  3,357.    Elapsed: 0:04:21.\n",
      "  Batch   960  of  3,357.    Elapsed: 0:04:30.\n",
      "  Batch 1,000  of  3,357.    Elapsed: 0:04:40.\n",
      "  Batch 1,040  of  3,357.    Elapsed: 0:04:49.\n",
      "  Batch 1,080  of  3,357.    Elapsed: 0:04:58.\n",
      "  Batch 1,120  of  3,357.    Elapsed: 0:05:21.\n",
      "  Batch 1,160  of  3,357.    Elapsed: 0:05:32.\n",
      "  Batch 1,200  of  3,357.    Elapsed: 0:05:41.\n",
      "  Batch 1,240  of  3,357.    Elapsed: 0:05:50.\n",
      "  Batch 1,280  of  3,357.    Elapsed: 0:06:00.\n",
      "  Batch 1,320  of  3,357.    Elapsed: 0:06:09.\n",
      "  Batch 1,360  of  3,357.    Elapsed: 0:06:18.\n",
      "  Batch 1,400  of  3,357.    Elapsed: 0:06:27.\n",
      "  Batch 1,440  of  3,357.    Elapsed: 0:06:40.\n",
      "  Batch 1,480  of  3,357.    Elapsed: 0:07:03.\n",
      "  Batch 1,520  of  3,357.    Elapsed: 0:07:12.\n",
      "  Batch 1,560  of  3,357.    Elapsed: 0:07:22.\n",
      "  Batch 1,600  of  3,357.    Elapsed: 0:07:31.\n",
      "  Batch 1,640  of  3,357.    Elapsed: 0:07:40.\n",
      "  Batch 1,680  of  3,357.    Elapsed: 0:07:49.\n",
      "  Batch 1,720  of  3,357.    Elapsed: 0:07:58.\n",
      "  Batch 1,760  of  3,357.    Elapsed: 0:08:08.\n",
      "  Batch 1,800  of  3,357.    Elapsed: 0:08:33.\n",
      "  Batch 1,840  of  3,357.    Elapsed: 0:08:43.\n",
      "  Batch 1,880  of  3,357.    Elapsed: 0:08:52.\n",
      "  Batch 1,920  of  3,357.    Elapsed: 0:09:01.\n",
      "  Batch 1,960  of  3,357.    Elapsed: 0:09:10.\n",
      "  Batch 2,000  of  3,357.    Elapsed: 0:09:19.\n",
      "  Batch 2,040  of  3,357.    Elapsed: 0:09:28.\n",
      "  Batch 2,080  of  3,357.    Elapsed: 0:09:38.\n",
      "  Batch 2,120  of  3,357.    Elapsed: 0:10:04.\n",
      "  Batch 2,160  of  3,357.    Elapsed: 0:10:13.\n",
      "  Batch 2,200  of  3,357.    Elapsed: 0:10:23.\n",
      "  Batch 2,240  of  3,357.    Elapsed: 0:10:32.\n",
      "  Batch 2,280  of  3,357.    Elapsed: 0:10:41.\n",
      "  Batch 2,320  of  3,357.    Elapsed: 0:10:50.\n",
      "  Batch 2,360  of  3,357.    Elapsed: 0:10:59.\n",
      "  Batch 2,400  of  3,357.    Elapsed: 0:11:09.\n",
      "  Batch 2,440  of  3,357.    Elapsed: 0:11:33.\n",
      "  Batch 2,480  of  3,357.    Elapsed: 0:11:42.\n",
      "  Batch 2,520  of  3,357.    Elapsed: 0:11:51.\n",
      "  Batch 2,560  of  3,357.    Elapsed: 0:12:01.\n",
      "  Batch 2,600  of  3,357.    Elapsed: 0:12:10.\n",
      "  Batch 2,640  of  3,357.    Elapsed: 0:12:19.\n",
      "  Batch 2,680  of  3,357.    Elapsed: 0:12:29.\n",
      "  Batch 2,720  of  3,357.    Elapsed: 0:12:38.\n",
      "  Batch 2,760  of  3,357.    Elapsed: 0:12:47.\n",
      "  Batch 2,800  of  3,357.    Elapsed: 0:13:11.\n",
      "  Batch 2,840  of  3,357.    Elapsed: 0:13:21.\n",
      "  Batch 2,880  of  3,357.    Elapsed: 0:13:30.\n",
      "  Batch 2,920  of  3,357.    Elapsed: 0:13:39.\n",
      "  Batch 2,960  of  3,357.    Elapsed: 0:13:48.\n",
      "  Batch 3,000  of  3,357.    Elapsed: 0:13:57.\n",
      "  Batch 3,040  of  3,357.    Elapsed: 0:14:07.\n",
      "  Batch 3,080  of  3,357.    Elapsed: 0:14:20.\n",
      "  Batch 3,120  of  3,357.    Elapsed: 0:14:39.\n",
      "  Batch 3,160  of  3,357.    Elapsed: 0:14:48.\n",
      "  Batch 3,200  of  3,357.    Elapsed: 0:14:57.\n",
      "  Batch 3,240  of  3,357.    Elapsed: 0:15:07.\n",
      "  Batch 3,280  of  3,357.    Elapsed: 0:15:16.\n",
      "  Batch 3,320  of  3,357.    Elapsed: 0:15:25.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:15:32\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  3,357.    Elapsed: 0:00:08.\n",
      "  Batch    80  of  3,357.    Elapsed: 0:00:17.\n",
      "  Batch   120  of  3,357.    Elapsed: 0:00:42.\n",
      "  Batch   160  of  3,357.    Elapsed: 0:00:51.\n",
      "  Batch   200  of  3,357.    Elapsed: 0:00:59.\n",
      "  Batch   240  of  3,357.    Elapsed: 0:01:07.\n",
      "  Batch   280  of  3,357.    Elapsed: 0:01:16.\n",
      "  Batch   320  of  3,357.    Elapsed: 0:01:24.\n",
      "  Batch   360  of  3,357.    Elapsed: 0:01:33.\n",
      "  Batch   400  of  3,357.    Elapsed: 0:01:41.\n",
      "  Batch   440  of  3,357.    Elapsed: 0:01:51.\n",
      "  Batch   480  of  3,357.    Elapsed: 0:02:00.\n",
      "  Batch   520  of  3,357.    Elapsed: 0:02:10.\n",
      "  Batch   560  of  3,357.    Elapsed: 0:02:35.\n",
      "  Batch   600  of  3,357.    Elapsed: 0:02:44.\n",
      "  Batch   640  of  3,357.    Elapsed: 0:02:53.\n",
      "  Batch   680  of  3,357.    Elapsed: 0:03:02.\n",
      "  Batch   720  of  3,357.    Elapsed: 0:03:11.\n",
      "  Batch   760  of  3,357.    Elapsed: 0:03:20.\n",
      "  Batch   800  of  3,357.    Elapsed: 0:03:30.\n",
      "  Batch   840  of  3,357.    Elapsed: 0:03:39.\n",
      "  Batch   880  of  3,357.    Elapsed: 0:04:04.\n",
      "  Batch   920  of  3,357.    Elapsed: 0:04:13.\n",
      "  Batch   960  of  3,357.    Elapsed: 0:04:23.\n",
      "  Batch 1,000  of  3,357.    Elapsed: 0:04:32.\n",
      "  Batch 1,040  of  3,357.    Elapsed: 0:04:41.\n",
      "  Batch 1,080  of  3,357.    Elapsed: 0:04:50.\n",
      "  Batch 1,120  of  3,357.    Elapsed: 0:05:00.\n",
      "  Batch 1,160  of  3,357.    Elapsed: 0:05:18.\n",
      "  Batch 1,200  of  3,357.    Elapsed: 0:05:33.\n",
      "  Batch 1,240  of  3,357.    Elapsed: 0:05:43.\n",
      "  Batch 1,280  of  3,357.    Elapsed: 0:05:52.\n",
      "  Batch 1,320  of  3,357.    Elapsed: 0:06:01.\n",
      "  Batch 1,360  of  3,357.    Elapsed: 0:06:10.\n",
      "  Batch 1,400  of  3,357.    Elapsed: 0:06:19.\n",
      "  Batch 1,440  of  3,357.    Elapsed: 0:06:29.\n",
      "  Batch 1,480  of  3,357.    Elapsed: 0:06:38.\n",
      "  Batch 1,520  of  3,357.    Elapsed: 0:07:03.\n",
      "  Batch 1,560  of  3,357.    Elapsed: 0:07:13.\n",
      "  Batch 1,600  of  3,357.    Elapsed: 0:07:22.\n",
      "  Batch 1,640  of  3,357.    Elapsed: 0:07:31.\n",
      "  Batch 1,680  of  3,357.    Elapsed: 0:07:40.\n",
      "  Batch 1,720  of  3,357.    Elapsed: 0:07:50.\n",
      "  Batch 1,760  of  3,357.    Elapsed: 0:07:59.\n",
      "  Batch 1,800  of  3,357.    Elapsed: 0:08:08.\n",
      "  Batch 1,840  of  3,357.    Elapsed: 0:08:34.\n",
      "  Batch 1,880  of  3,357.    Elapsed: 0:08:43.\n",
      "  Batch 1,920  of  3,357.    Elapsed: 0:08:52.\n",
      "  Batch 1,960  of  3,357.    Elapsed: 0:09:02.\n",
      "  Batch 2,000  of  3,357.    Elapsed: 0:09:11.\n",
      "  Batch 2,040  of  3,357.    Elapsed: 0:09:20.\n",
      "  Batch 2,080  of  3,357.    Elapsed: 0:09:29.\n",
      "  Batch 2,120  of  3,357.    Elapsed: 0:09:39.\n",
      "  Batch 2,160  of  3,357.    Elapsed: 0:09:59.\n",
      "  Batch 2,200  of  3,357.    Elapsed: 0:10:13.\n",
      "  Batch 2,240  of  3,357.    Elapsed: 0:10:22.\n",
      "  Batch 2,280  of  3,357.    Elapsed: 0:10:31.\n",
      "  Batch 2,320  of  3,357.    Elapsed: 0:10:40.\n",
      "  Batch 2,360  of  3,357.    Elapsed: 0:10:49.\n",
      "  Batch 2,400  of  3,357.    Elapsed: 0:10:59.\n",
      "  Batch 2,440  of  3,357.    Elapsed: 0:11:11.\n",
      "  Batch 2,480  of  3,357.    Elapsed: 0:11:33.\n",
      "  Batch 2,520  of  3,357.    Elapsed: 0:11:42.\n",
      "  Batch 2,560  of  3,357.    Elapsed: 0:11:51.\n",
      "  Batch 2,600  of  3,357.    Elapsed: 0:12:01.\n",
      "  Batch 2,640  of  3,357.    Elapsed: 0:12:10.\n",
      "  Batch 2,680  of  3,357.    Elapsed: 0:12:19.\n",
      "  Batch 2,720  of  3,357.    Elapsed: 0:12:28.\n",
      "  Batch 2,760  of  3,357.    Elapsed: 0:12:37.\n",
      "  Batch 2,800  of  3,357.    Elapsed: 0:13:03.\n",
      "  Batch 2,840  of  3,357.    Elapsed: 0:13:12.\n",
      "  Batch 2,880  of  3,357.    Elapsed: 0:13:21.\n",
      "  Batch 2,920  of  3,357.    Elapsed: 0:13:30.\n",
      "  Batch 2,960  of  3,357.    Elapsed: 0:13:39.\n",
      "  Batch 3,000  of  3,357.    Elapsed: 0:13:48.\n",
      "  Batch 3,040  of  3,357.    Elapsed: 0:13:57.\n",
      "  Batch 3,080  of  3,357.    Elapsed: 0:14:07.\n",
      "  Batch 3,120  of  3,357.    Elapsed: 0:14:31.\n",
      "  Batch 3,160  of  3,357.    Elapsed: 0:14:41.\n",
      "  Batch 3,200  of  3,357.    Elapsed: 0:14:50.\n",
      "  Batch 3,240  of  3,357.    Elapsed: 0:14:59.\n",
      "  Batch 3,280  of  3,357.    Elapsed: 0:15:08.\n",
      "  Batch 3,320  of  3,357.    Elapsed: 0:15:17.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:15:26\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "loss_values = []\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "        with autocast():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.83      0.84      0.84     25554\n",
      "     Class 1       0.84      0.83      0.84     25554\n",
      "\n",
      "    accuracy                           0.84     51108\n",
      "   macro avg       0.84      0.84      0.84     51108\n",
      "weighted avg       0.84      0.84      0.84     51108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for batch in validation_dataloader:\n",
    "   \n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "  \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "preds = np.argmax(predictions, axis=1)\n",
    "\n",
    "class_report = classification_report(true_labels, preds, target_names=['Class 0', 'Class 1'])\n",
    "\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB,LGBM,CATBOOST without balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier Metrics:\n",
      "Accuracy: 0.82\n",
      "Precision: 0.65\n",
      "Recall: 0.52\n",
      "F1 Score: 0.58\n",
      "ROC AUC: 0.85\n",
      "------------------------------\n",
      "[LightGBM] [Info] Number of positive: 59672, number of negative: 192089\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4415\n",
      "[LightGBM] [Info] Number of data points in the train set: 251761, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.237018 -> initscore=-1.169096\n",
      "[LightGBM] [Info] Start training from score -1.169096\n",
      "LGBMClassifier Metrics:\n",
      "Accuracy: 0.84\n",
      "Precision: 0.72\n",
      "Recall: 0.50\n",
      "F1 Score: 0.59\n",
      "ROC AUC: 0.88\n",
      "------------------------------\n",
      "CatBoostClassifier Metrics:\n",
      "Accuracy: 0.83\n",
      "Precision: 0.65\n",
      "Recall: 0.63\n",
      "F1 Score: 0.64\n",
      "ROC AUC: 0.88\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8)\n",
    "lgbm_model = LGBMClassifier(boosting_type='gbdt', learning_rate=0.1, n_estimators=200, num_leaves=127)\n",
    "catboost_model = CatBoostClassifier(verbose=0, depth=5, iterations=200, learning_rate=0.5)\n",
    "\n",
    "models = [xgb_model, lgbm_model, catboost_model]\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] \n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"{model_name} Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# same 3 models(excluding transformer) as above with balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier Metrics:\n",
      "Accuracy: 0.77\n",
      "Precision: 0.74\n",
      "Recall: 0.82\n",
      "F1 Score: 0.78\n",
      "ROC AUC: 0.85\n",
      "------------------------------\n",
      "[LightGBM] [Info] Number of positive: 59672, number of negative: 59672\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4088\n",
      "[LightGBM] [Info] Number of data points in the train set: 119344, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "LGBMClassifier Metrics:\n",
      "Accuracy: 0.79\n",
      "Precision: 0.79\n",
      "Recall: 0.80\n",
      "F1 Score: 0.80\n",
      "ROC AUC: 0.88\n",
      "------------------------------\n",
      "CatBoostClassifier Metrics:\n",
      "Accuracy: 0.79\n",
      "Precision: 0.76\n",
      "Recall: 0.86\n",
      "F1 Score: 0.80\n",
      "ROC AUC: 0.88\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8)\n",
    "lgbm_model = LGBMClassifier(boosting_type='gbdt', learning_rate=0.1, n_estimators=200, num_leaves=127)\n",
    "catboost_model = CatBoostClassifier(verbose=0, depth=5, iterations=200, learning_rate=0.5)\n",
    "\n",
    "models = [xgb_model, lgbm_model, catboost_model]\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    y_pred = model.predict(X_test_balanced)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_balanced, y_pred)\n",
    "    precision = precision_score(y_test_balanced, y_pred)\n",
    "    recall = recall_score(y_test_balanced, y_pred)\n",
    "    f1 = f1_score(y_test_balanced, y_pred)\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_test_balanced)[:, 1] \n",
    "    roc_auc = roc_auc_score(y_test_balanced, y_pred_proba)\n",
    "    \n",
    "    print(f\"{model_name} Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
